{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Indox Retrieval Augmentation\n"
   ],
   "id": "f454fd6293a873f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here, we will explore how to work with Indox Retrieval Augmentation. First, if you are using OpenAI, you should set your OpenAI key as an environment variable.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDox/blob/master/Demo/openai_unstructured.ipynb)"
   ],
   "id": "c67508fa389e569f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install openai\n",
    "!pip install indox\n",
    "!pip install chromadb\n",
    "!pip install duckduckgo-search"
   ],
   "id": "fa7a1f15146dc5fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setting Up the Python Environment\n",
    "\n",
    "If you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indox`:\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "```bash\n",
    "python -m venv indox\n",
    "```\n",
    "2. **Activate the virtual environment:**\n",
    "```bash\n",
    "indox_judge\\Scripts\\activate\n",
    "```\n",
    "\n",
    "### macOS/Linux\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "   ```bash\n",
    "   python3 -m venv indox\n",
    "```\n",
    "\n",
    "2. **Activate the virtual environment:**\n",
    "    ```bash\n",
    "   source indox/bin/activate\n",
    "```\n",
    "### Install Dependencies\n",
    "\n",
    "Once the virtual environment is activated, install the required dependencies by running:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ],
   "id": "fdf2eada68410ff3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:10.365320Z",
     "start_time": "2024-07-12T12:59:10.353525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ],
   "id": "88e8c38ba3b8886d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Creating an instance of IndoxRetrivalAugmentation\n",
    "\n",
    "To effectively utilize the Indox Retrieval Augmentation capabilities, you must first create an instance of the IndoxRetrievalAugmentation class. This instance will allow you to access the methods and properties defined within the class, enabling the augmentation and retrieval functionalities."
   ],
   "id": "ac995737f9b2fe6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:11.258783Z",
     "start_time": "2024-07-12T12:59:11.195155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox import IndoxRetrievalAugmentation\n",
    "\n",
    "indox = IndoxRetrievalAugmentation()"
   ],
   "id": "131692307c9154db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mIndoxRetrievalAugmentation initialized\u001B[0m\n",
      "\n",
      "            ██  ███    ██  ██████   ██████  ██       ██\n",
      "            ██  ████   ██  ██   ██ ██    ██   ██  ██\n",
      "            ██  ██ ██  ██  ██   ██ ██    ██     ██\n",
      "            ██  ██  ██ ██  ██   ██ ██    ██   ██   ██\n",
      "            ██  ██  █████  ██████   ██████  ██       ██\n",
      "            \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generating response using OpenAI's language models \n",
    "OpenAIQA class is used to handle question-answering task using OpenAI's language models. This instance creates OpenAiEmbedding class to specifying embedding model. Here ChromaVectorStore handles the storage and retrieval of vector embeddings by specifying a collection name and sets up a vector store where text embeddings can be stored and queried."
   ],
   "id": "759db8f502cbd91f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:16.953419Z",
     "start_time": "2024-07-12T12:59:12.210458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.llms import OpenAi\n",
    "from indox.embeddings import OpenAiEmbedding\n",
    "\n",
    "openai_qa = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n",
    "embed_openai = OpenAiEmbedding(api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")\n",
    "\n",
    "from indox.vector_stores import Chroma\n",
    "\n",
    "db = Chroma(collection_name=\"sample\", embedding_function=embed_openai)"
   ],
   "id": "f32d98545c6d3c3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mInitializing OpenAi with model: gpt-3.5-turbo-0125\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mOpenAi initialized successfully\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mInitialized OpenAI embeddings with model: text-embedding-3-small\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mConnection to the vector store database established successfully\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<indox.vector_stores.Chroma.ChromaVectorStore at 0x249ce6666c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### load and preprocess data\n",
    "This part of code demonstrates how to load and preprocess text data from a file, split it into chunks, and store these chunks in the vector store that was set up previously."
   ],
   "id": "1e3408e8f8a8ad17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt",
   "id": "ab5ee3b881d4be8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:16.957329Z",
     "start_time": "2024-07-12T12:59:16.954425Z"
    }
   },
   "cell_type": "code",
   "source": "file_path = \"sample.txt\"",
   "id": "1d8e56a9f88e03cf",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:37.453098Z",
     "start_time": "2024-07-12T12:59:16.958336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.data_loader_splitter import UnstructuredLoadAndSplit\n",
    "\n",
    "loader_splitter = UnstructuredLoadAndSplit(file_path=file_path, max_chunk_size=400, remove_sword=False)\n",
    "docs = loader_splitter.load_and_chunk()"
   ],
   "id": "827c44ce67f972c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mUnstructuredLoadAndSplit initialized successfully\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mGetting all documents\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mStarting processing\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mUsing title-based chunking\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted chunking process\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mSuccessfully obtained all documents\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:37.458989Z",
     "start_time": "2024-07-12T12:59:37.454106Z"
    }
   },
   "cell_type": "code",
   "source": "docs[0].page_content",
   "id": "82af3fb1c9f5643a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The wife of a rich man fell sick, and as she felt that her endwas drawing near, she called her only daughter to her bedside andsaid, dear child, be good and pious, and then thegood God will always protect you, and I will look down on youfrom heaven and be near you. Thereupon she closed her eyes anddeparted. Every day the maiden went out to her mother's grave,\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:45.534128Z",
     "start_time": "2024-07-12T12:59:37.459999Z"
    }
   },
   "cell_type": "code",
   "source": "db.add(docs=docs)",
   "id": "4557891dec337e31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mStoring documents in the vector store\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mDocument added successfully to the vector store.\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mDocuments stored successfully\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<indox.vector_stores.Chroma.ChromaVectorStore at 0x249ce6666c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Retrieve relevant information and generate an answer\n",
    "The main purpose of these lines is to perform a query on the vector store to retrieve the most relevant information (top_k=5) and generate an answer using the language model."
   ],
   "id": "cd6bd4924ad116fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:45.538294Z",
     "start_time": "2024-07-12T12:59:45.534646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"How Cinderella reach her happy ending?\"\n",
    "retriever = indox.QuestionAnswer(vector_database=db, llm=openai_qa, top_k=5)"
   ],
   "id": "593ec3a85c796115",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "invoke(query) method sends the query to the retriever, which searches the vector store for relevant text chunks and uses the language model to generate a response based on the retrieved information.\n",
    "Context property retrieves the context or the detailed information that the retriever used to generate the answer to the query. It provides insight into how the query was answered by showing the relevant text chunks and any additional information used."
   ],
   "id": "9e778403c8d864c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:52.302283Z",
     "start_time": "2024-07-12T12:59:45.539299Z"
    }
   },
   "cell_type": "code",
   "source": "retriever.invoke(query)",
   "id": "60a2d55199cf0ce6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mRetrieving context and scores from the vector database\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mGenerating answer without document relevancy filter\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mAnswering question\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mGenerating response\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mResponse generated successfully\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mQuery answered successfully\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Cinderella reached her happy ending by being kind, patient, and having a pure heart. Despite facing mistreatment from her step-family, she remained humble and continued to do good deeds. With the help of a little white bird and magical assistance, Cinderella was able to attend the royal festival where the prince fell in love with her. Ultimately, her kindness, inner beauty, and resilience led her to marry the prince and live happily ever after.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:52.307744Z",
     "start_time": "2024-07-12T12:59:52.303288Z"
    }
   },
   "cell_type": "code",
   "source": "retriever.context",
   "id": "58d79450c0807286",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"to appear among the number, they were delighted, called cinderellaand said, comb our hair for us, brush our shoes and fasten ourbuckles, for we are going to the wedding at the king's palace.Cinderella obeyed, but wept, because she too would have liked togo with them to the dance, and begged her step-mother to allowher to do so. You go, cinderella, said she, covered in dust and\",\n",
       " 'cinderella expressed a wish, the bird threw down to her what shehad wished for.It happened, however, that the king gave orders for a festivalwhich was to last three days, and to which all the beautiful younggirls in the country were invited, in order that his son might choosehimself a bride. When the two step-sisters heard that they too were',\n",
       " \"which they had wished for, and to cinderella he gave the branchfrom the hazel-bush. Cinderella thanked him, went to her mother'sgrave and planted the branch on it, and wept so much that the tearsfell down on it and watered it. And it grew and became a handsometree. Thrice a day cinderella went and sat beneath it, and wept andprayed, and a little white bird always came on the tree, and if\",\n",
       " 'by the hearth in the cinders. And as on that account she alwayslooked dusty and dirty, they called her cinderella.It happened that the father was once going to the fair, and heasked his two step-daughters what he should bring back for them.Beautiful dresses, said one, pearls and jewels, said the second.And you, cinderella, said he, what will you have. Father',\n",
       " 'the two sisters were horrified and became pale with rage, he,however, took cinderella on his horse and rode away with her. Asthey passed by the hazel-tree, the two white doves cried -turn and peep, turn and peep,no blood is in the shoe,the shoe is not too small for her,the true bride rides with you,and when they had cried that, the two came flying down and']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### With AgenticRag\n",
    "\n",
    "AgenticRag stands for Agentic Retrieval-Augmented Generation. This concept combines retrieval-based methods and generation-based methods in natural language processing (NLP). The key idea is to enhance the generative capabilities of a language model by incorporating relevant information retrieved from a database or a vector store. \n",
    " AgenticRag is designed to provide more contextually rich and accurate responses by utilizing external knowledge sources. It retrieves relevant pieces of information (chunks) from a vector store based on a query and then uses a language model to generate a comprehensive response that incorporates this retrieved information."
   ],
   "id": "486878b3d49c9871"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T14:05:45.072129Z",
     "start_time": "2024-06-18T14:05:27.607158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = indox.AgenticRag(llm=openai_qa, vector_database=db, top_k=5)\n",
    "agent.run(query)"
   ],
   "id": "394533a0e6ab8228",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 17:35:30,845 INFO:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 17:35:33,908 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 17:35:34,935 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 17:35:36,219 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Relevant doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 17:35:36,984 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 17:35:38,100 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Relevant doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 17:35:38,103 INFO:Answering question: How Cinderella reach her happy ending?\n",
      "2024-06-18 17:35:38,103 INFO:Attempting to generate an answer for the question: How Cinderella reach her happy ending?\n",
      "2024-06-18 17:35:40,757 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 17:35:40,758 INFO:Answer generated successfully\n",
      "2024-06-18 17:35:42,148 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 17:35:42,149 INFO:Agent answer generated successfully\n",
      "2024-06-18 17:35:42,149 INFO:Hallucination detected, Regenerate the answer...\n",
      "2024-06-18 17:35:42,150 INFO:Answering question: How Cinderella reach her happy ending?\n",
      "2024-06-18 17:35:42,150 INFO:Attempting to generate an answer for the question: How Cinderella reach her happy ending?\n",
      "2024-06-18 17:35:45,066 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 17:35:45,068 INFO:Answer generated successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Cinderella reached her happy ending by receiving help from the hazel tree, which grew from the branch given to her by the prince. She planted the branch on her mother's grave, and the tree grew into a handsome tree. Cinderella would sit beneath the tree, weep, and pray, and a little white bird would always come to her. This bird helped Cinderella by providing her with the beautiful dresses and shoes she needed to attend the wedding at the king's palace. Ultimately, with the help of the magical tree and the little white bird, Cinderella was able to overcome the obstacles set by her stepmother and stepsisters and attend the royal wedding, leading to her happy ending.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T14:05:54.605800Z",
     "start_time": "2024-06-18T14:05:54.602208Z"
    }
   },
   "cell_type": "code",
   "source": "query_2 = \"Where does Messi play right now?\"",
   "id": "98ac04439406a236",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T14:06:18.437557Z",
     "start_time": "2024-06-18T14:05:55.338390Z"
    }
   },
   "cell_type": "code",
   "source": "agent.run(query_2)",
   "id": "bc91f9296129df1d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 17:35:58,307 INFO:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 17:36:00,728 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Relevant doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 17:36:01,635 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Relevant doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 17:36:03,147 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Relevant doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 17:36:04,179 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Relevant doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 17:36:05,349 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Relevant doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 17:36:05,351 INFO:No Relevant document found, Start web search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Relevant Context Found, Start Searching On Web...\n",
      "Answer Base On Web Search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 17:36:13,280 INFO:Answering question: Where does Messi play right now?\n",
      "2024-06-18 17:36:13,281 INFO:Attempting to generate an answer for the question: Where does Messi play right now?\n",
      "2024-06-18 17:36:15,879 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 17:36:15,880 INFO:Answer generated successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check For Hallucination In Generated Answer Base On Web Search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 17:36:17,221 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 17:36:17,222 INFO:Agent answer generated successfully\n",
      "2024-06-18 17:36:17,223 INFO:Hallucination detected, Regenerate the answer...\n",
      "2024-06-18 17:36:17,223 INFO:Answering question: Where does Messi play right now?\n",
      "2024-06-18 17:36:17,224 INFO:Attempting to generate an answer for the question: Where does Messi play right now?\n",
      "2024-06-18 17:36:18,432 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 17:36:18,434 INFO:Answer generated successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Messi currently plays for Major League Soccer's Inter Miami CF.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
