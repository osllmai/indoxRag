{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Using GithubReader for Accessing Files from GitHub\n",
    "In this notebook, we will demonstrate how to use the `GithubReader` class for accessing files from GitHub repositories. The `GithubReader` class interacts with the GitHub API to retrieve file content and metadata, which can be useful for various applications including research and question-answering systems.\n",
    "\n",
    "To begin, ensure you have your GitHub personal access token ready for authentication. This token is crucial for accessing private repositories and ensuring secure interactions with the GitHub API.\n",
    "\n",
    "Also, ensure you have set up your environment variables and API keys in Python using the dotenv library. This is crucial for securely managing sensitive information, such as API keys, especially when using services like HuggingFace. Ensure your `HUGGINGFACE_API_KEY` is defined in the `.env` file to avoid hardcoding sensitive data into your codebase, thus enhancing security and maintainability.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDox/blob/master/Demo/github_chroma_recursive.ipynb)"
   ],
   "id": "a2cefe0bb87e8f0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install indox\n",
    "!pip install chromadb\n",
    "!pip install pygithub\n",
    "!pip install sentence_transformers\n",
    "!pip install chromadb"
   ],
   "id": "62f10230b002c074"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setting Up the Python Environment\n",
    "\n",
    "If you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indox`:\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "```bash\n",
    "python -m venv indox\n",
    "```\n",
    "2. **Activate the virtual environment:**\n",
    "```bash\n",
    "indox_judge\\Scripts\\activate\n",
    "```\n",
    "\n",
    "### macOS/Linux\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "   ```bash\n",
    "   python3 -m venv indox\n",
    "    ```\n",
    "\n",
    "2. **Activate the virtual environment:**\n",
    "    ```bash\n",
    "   source indox/bin/activate\n",
    "    ```\n",
    "   \n",
    "### Install Dependencies\n",
    "\n",
    "Once the virtual environment is activated, install the required dependencies by running:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ],
   "id": "24e48eaa6aaba881"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Import Essential Libraries\n",
    "\n",
    "Next, we import the essential libraries for our Indox question-answering system:\n",
    "\n",
    "- `IndoxRetrievalAugmentation`: Enhances the retrieval process by improving the relevance and quality of the documents retrieved, leading to better QA performance.\n",
    "- `MistralQA`: A powerful QA model provided by Indox, built on top of the Hugging Face model architecture. It leverages state-of-the-art language understanding to deliver precise answers.\n",
    "- `HuggingFaceEmbedding`: This library uses Hugging Face embeddings to enrich semantic understanding, making it easier to capture the contextual meaning of the text.\n",
    "- `RecursiveCharacterTextSplitter`: Utilizes a recursive approach to divide large text documents into smaller chunks based on character length and semantic boundaries, ensuring that each segment maintains contextual integrity"
   ],
   "id": "db8f4fda78fb1dce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:16:10.007626Z",
     "start_time": "2024-08-25T14:16:09.969100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('api.env')\n",
    "\n",
    "HUGGINGFACE_API_KEY = os.environ['HUGGINGFACE_API_KEY']\n",
    "github_token = os.environ['github_token']"
   ],
   "id": "dfc9daa14417b6e9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-25T14:16:11.505804Z",
     "start_time": "2024-08-25T14:16:11.452176Z"
    }
   },
   "source": [
    "from indox import IndoxRetrievalAugmentation\n",
    "indox = IndoxRetrievalAugmentation()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mIndoxRetrievalAugmentation initialized\u001B[0m\n",
      "\n",
      "            ██  ███    ██  ██████   ██████  ██       ██\n",
      "            ██  ████   ██  ██   ██ ██    ██   ██  ██\n",
      "            ██  ██ ██  ██  ██   ██ ██    ██     ██\n",
      "            ██  ██  ██ ██  ██   ██ ██    ██   ██   ██\n",
      "            ██  ██  █████  ██████   ██████  ██       ██\n",
      "            \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Building the GithubReader System and Initializing Models\n",
    "\n",
    "Next, we will build our GithubReader system and initialize the necessary models for processing GitHub repository content. This setup will enable us to effectively retrieve and handle files from GitHub, leveraging these models to support various research and question-answering tasks.\n"
   ],
   "id": "f7796775b38f00ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:16:24.023990Z",
     "start_time": "2024-08-25T14:16:14.070679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.llms import HuggingFaceModel\n",
    "from indox.embeddings import HuggingFaceEmbedding\n",
    "mistral_qa = HuggingFaceModel(api_key=HUGGINGFACE_API_KEY,model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "embed = HuggingFaceEmbedding(api_key=HUGGINGFACE_API_KEY,model=\"multi-qa-mpnet-base-cos-v1\")"
   ],
   "id": "aad9a7a29fc81465",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mInitializing HuggingFaceModel with model: mistralai/Mistral-7B-Instruct-v0.2\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mHuggingFaceModel initialized successfully\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mInitialized HuggingFaceEmbedding with model: multi-qa-mpnet-base-cos-v1\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setting Up the GithubReader for Retrieving Repository Content\n",
    "To demonstrate the capabilities of our `GithubReader` system and its integration with `Indox`, we will use a sample GitHub repository. This repository will contain reference data, such as various files and documents, which we will use for testing and evaluation."
   ],
   "id": "3fef77b68e2fd805"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:16:43.955996Z",
     "start_time": "2024-08-25T14:16:24.024990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.data_connectors import GithubClient, GithubRepositoryReader\n",
    "\n",
    "github_client = GithubClient(github_token=github_token)\n",
    "\n",
    "repo_reader = GithubRepositoryReader(\n",
    "    github_client=github_client,\n",
    "    owner=\"osllmai\",\n",
    "    repo=\"indoxjudge\",\n",
    "    filter_directories=([\"docs\"], GithubRepositoryReader.FilterType.INCLUDE),\n",
    "    filter_file_extensions=([\".md\"], GithubRepositoryReader.FilterType.INCLUDE)\n",
    ")\n",
    "\n",
    "documents = repo_reader.load_content(branch=\"main\")"
   ],
   "id": "ff2ba25cec5cb1a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:16:45.377233Z",
     "start_time": "2024-08-25T14:16:45.372232Z"
    }
   },
   "cell_type": "code",
   "source": "content = documents\n",
   "id": "e1f7b24835386cdb",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:16:47.406613Z",
     "start_time": "2024-08-25T14:16:47.383311Z"
    }
   },
   "cell_type": "code",
   "source": "content",
   "id": "2b7dd4966f2aa035",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Branch Naming and Pull Request Guidelines for the Team\\n\\n### Note 1: Branch Naming\\n\\nPay attention to the type of task assigned to you. Is it a feature, a bug, or a refactor?\\n\\n- If it\\'s a bug: The branch name should start with the word \"issue\".\\n- If it\\'s a feature: The branch name should start with the word \"feature\".\\n- If it\\'s a refactor: The branch name should start with the word \"refactor\".\\n- If it\\'s for documentation : The branch name should start with the word \"docs\".\\n### Note 2: Creating a Pull Request\\n\\nFor every branch you create, you need to make a pull request at the end of development. However, there are some rules:\\n\\n1. Ensure your code adheres to a set of technical guidelines before creating the pull request. This includes following coding standards and running all necessary tests.\\n2. Write detailed descriptions for the pull request. This should include  an explanation of the issue solved and what you did.\\n3. Limit your changes to no more than 10 files to make the review process easier. If there are more changes, split them into multiple branches and pull requests.\\n4. At least one review and approval are necessary before you can merge the pull request.\\n5. It\\'s best if the whole team reviews the code. If someone thinks the code can be improved, they should comment. If the comment is reasonable, the request owner should update the code. If they can\\'t agree, they should hold a meeting with other team members to discuss and present their reasons. After any corrections, the code is reviewed again. If there are no further issues, it gets approved, the branch is merged, and then the branch is deleted.\\n6. After merging, make sure to clean up by deleting the branch to keep the repository tidy.\\n',\n",
       " '<p align=\"center\">\\n\\n\\n<div style=\"position: relative; width: 100%; text-align: center;\">\\n    <h1>inDoxJudge</h1>\\n    <a href=\"https://github.com/osllmai/inDoxJudge\">\\n<img src=\"https://readme-typing-svg.demolab.com?font=Georgia&size=16&duration=3000&pause=500&multiline=true&width=700&height=100&lines=InDoxJudge;LLM+Evaluation+%7C+RAG+Evaluation+%7C+Safety+Evaluation+%7C+LLM+Comparison;Copyright+©️+OSLLAM.ai\" alt=\"Typing SVG\" style=\"margin-top: 20px;\"/>\\n    </a>\\n</div>\\n\\n</br>\\n\\n[![License](https://img.shields.io/github/license/osllmai/inDox)](https://github.com/osllmai/inDox/blob/main/LICENSE)\\n[![PyPI](https://badge.fury.io/py/indoxJudge.svg)](https://pypi.org/project/IndoxJudge/0.0.1/)\\n[![Python](https://img.shields.io/pypi/pyversions/indoxJudge.svg)](https://pypi.org/project/indoxJudge/0.0.1/)\\n[![Downloads](https://static.pepy.tech/badge/indoxJudge)](https://pepy.tech/project/indoxJudge)\\n\\n[![Discord](https://img.shields.io/discord/1223867382460579961?label=Discord&logo=Discord&style=social)](https://discord.com/invite/ossllmai)\\n[![GitHub stars](https://img.shields.io/github/stars/osllmai/inDoxJudge?style=social)](https://github.com/osllmai/inDoxJudge)\\n\\n\\n\\n\\n<p align=\"center\">\\n  <a href=\"https://osllm.ai\">Official Website</a> &bull; <a href=\"https://github.com/osllmai/inDox/wiki\">Documentation</a> &bull; <a href=\"https://discord.gg/qrCc56ZR\">Discord</a>\\n</p>\\n\\n\\n<p align=\"center\">\\n  <b>NEW:</b> <a href=\"https://docs.google.com/forms/d/1CQXJvxLUqLBSXnjqQmRpOyZqD6nrKubLz2WTcIJ37fU/prefill\">Subscribe to our mailing list</a> for updates and news!\\n</p>\\n\\n\\nWelcome to IndoxJudge! This repository provides a comprehensive suite of evaluation metrics for assessing the performance and quality of large language models (LLMs). Whether you\\'re a researcher, developer, or enthusiast, this toolkit offers essential tools to measure various aspects of LLMs, including knowledge retention, bias, toxicity, and more.\\n\\n## Overview\\n\\nIndoxJudge is designed to provide a standardized and extensible framework for evaluating LLMs. With a focus on accuracy, fairness, and relevancy, this toolkit supports a wide range of evaluation metrics and is continuously updated to include the latest advancements in the field.\\n\\n## Features\\n\\n- **Comprehensive Metrics**: Evaluate LLMs across multiple dimensions, including accuracy, bias, toxicity, and contextual relevancy.\\n- **RAG Evaluation**: Includes specialized metrics for evaluating retrieval-augmented generation (RAG) models.\\n- **Safety Evaluation**: Assess the safety of model outputs, focusing on toxicity, bias, and ethical considerations.\\n- **Extensible Framework**: Easily integrate new metrics or customize existing ones to suit specific needs.\\n- **User-Friendly Interface**: Intuitive and easy-to-use interface for seamless evaluation.\\n- **Continuous Updates**: Regular updates to incorporate new metrics and improvements.\\n\\n## Supported Models\\n\\nIndoxJudge currently supports the following LLM models:\\n\\n- **OpenAi**\\n- **GoogleAi**\\n- **IndoxApi**\\n- **HuggingFaceModel**\\n- **Mistral**\\n- **Ollama**\\n\\n## Metrics\\n\\nIndoxJudge includes the following metrics, with more being added:\\n\\n- **GEval**: General evaluation metric for LLMs.\\n- **KnowledgeRetention**: Assesses the ability of LLMs to retain factual information.\\n- **BertScore**: Measures the similarity between generated and reference sentences.\\n- **Toxicity**: Evaluates the presence of toxic content in model outputs.\\n- **Bias**: Analyzes the potential biases in LLM outputs.\\n- **Hallucination**: Identifies instances where the model generates false or misleading information.\\n- **Faithfulness**: Checks the alignment of generated content with source material.\\n- **ContextualRelevancy**: Assesses the relevance of responses in context.\\n- **Rouge**: Measures the overlap of n-grams between generated and reference texts.\\n- **BLEU**: Evaluates the quality of text generation based on precision.\\n- **AnswerRelevancy**: Assesses the relevance of answers to questions.\\n- **METEOR**: Evaluates machine translation quality.\\n- **Gruen**: Measures the quality of generated text by assessing grammaticality, redundancy, and focus.\\n\\n## Installation\\n\\nTo install IndoxJudge, follow these steps:\\n\\n\\n```bash\\ngit clone https://github.com/yourusername/indoxjudge.git\\ncd indoxjudge\\n```\\n## Setting Up the Python Environment\\n\\nIf you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indox_judge`:\\n\\n### Windows\\n\\n1. **Create the virtual environment:**\\n```bash\\npython -m venv indox_judge\\n```\\n2. **Activate the virtual environment:**\\n```bash\\nindox_judge\\\\Scripts\\\\activate\\n```\\n\\n### macOS/Linux\\n\\n1. **Create the virtual environment:**\\n   ```bash\\n   python3 -m venv indox_judge\\n```\\n\\n2. **Activate the virtual environment:**\\n    ```bash\\n   source indox_judge/bin/activate\\n```\\n### Install Dependencies\\n\\nOnce the virtual environment is activated, install the required dependencies by running:\\n\\n```bash\\npip install -r requirements.txt\\n```\\n\\n\\n\\n\\n## Usage\\n\\nTo use IndoxJudge, load your API key, select the model, and choose the evaluation metrics. Here\\'s an example demonstrating how to evaluate a model\\'s response for faithfulness:\\n\\n```python\\nimport os\\nfrom dotenv import load_dotenv\\n\\n# Load environment variables\\nload_dotenv()\\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\\n\\n# Import IndoxJudge and supported models\\nfrom indoxJudge.piplines import CustomEvaluator\\nfrom indoxJudge.models import OpenAi\\nfrom indoxJudge.metrics import Faithfulness\\n\\n# Initialize the model with your API key\\nmodel = OpenAi(api_key=OPENAI_API_KEY,model=\"gpt-4o\")\\n\\n# Define your query and retrieval context\\nquery = \"What are the benefits of a Mediterranean diet?\"\\nretrieval_context = [\\n    \"The Mediterranean diet emphasizes eating primarily plant-based foods, such as fruits and vegetables, whole grains, legumes, and nuts. It also includes moderate amounts of fish and poultry, and low consumption of red meat. Olive oil is the main source of fat, providing monounsaturated fats which are beneficial for heart health.\",\\n    \"Research has shown that the Mediterranean diet can reduce the risk of heart disease, stroke, and type 2 diabetes. It is also associated with improved cognitive function and a lower risk of Alzheimer\\'s disease. The diet\\'s high content of fiber, antioxidants, and healthy fats contributes to its numerous health benefits.\",\\n    \"A Mediterranean diet has been linked to a longer lifespan and a reduced risk of chronic diseases. It promotes healthy aging and weight management due to its emphasis on whole, unprocessed foods and balanced nutrition.\"\\n]\\n\\n# Obtain the model\\'s response\\nresponse = \"The Mediterranean diet is known for its health benefits, including reducing the risk of heart disease, stroke, and diabetes. It encourages the consumption of fruits, vegetables, whole grains, nuts, and olive oil, while limiting red meat. Additionally, this diet has been associated with better cognitive function and a reduced risk of Alzheimer\\'s disease, promoting longevity and overall well-being.\"\\n\\n# Initialize the Faithfulness metric\\nfaithfulness_metrics = Faithfulness(llm_response=response, retrieval_context=retrieval_context)\\n\\n# Create an evaluator with the selected metrics\\nevaluator = CustomEvaluator(metrics=[faithfulness_metrics], model=model)\\n\\n# Evaluate the response\\nfaithfulness_result = evaluator.judge()\\n\\n# Output the evaluation result\\nprint(faithfulness_result)\\n```\\n## Example Output\\n\\n```json\\n{\\n  \"faithfulness\": {\\n    \"claims\": [\\n      \"The Mediterranean diet is known for its health benefits.\",\\n      \"The Mediterranean diet reduces the risk of heart disease.\",\\n      \"The Mediterranean diet reduces the risk of stroke.\",\\n      \"The Mediterranean diet reduces the risk of diabetes.\",\\n      \"The Mediterranean diet encourages the consumption of fruits.\",\\n      \"The Mediterranean diet encourages the consumption of vegetables.\",\\n      \"The Mediterranean diet encourages the consumption of whole grains.\",\\n      \"The Mediterranean diet encourages the consumption of nuts.\",\\n      \"The Mediterranean diet encourages the consumption of olive oil.\",\\n      \"The Mediterranean diet limits red meat consumption.\",\\n      \"The Mediterranean diet is associated with better cognitive function.\",\\n      \"The Mediterranean diet is associated with a reduced risk of Alzheimer\\'s disease.\",\\n      \"The Mediterranean diet promotes longevity.\",\\n      \"The Mediterranean diet promotes overall well-being.\"\\n    ],\\n    \"truths\": [\\n      \"The Mediterranean diet is known for its health benefits.\",\\n      \"The Mediterranean diet reduces the risk of heart disease, stroke, and diabetes.\",\\n      \"The Mediterranean diet encourages the consumption of fruits, vegetables, whole grains, nuts, and olive oil.\",\\n      \"The Mediterranean diet limits red meat consumption.\",\\n      \"The Mediterranean diet has been associated with better cognitive function.\",\\n      \"The Mediterranean diet has been associated with a reduced risk of Alzheimer\\'s disease.\",\\n      \"The Mediterranean diet promotes longevity and overall well-being.\"\\n    ],\\n    \"reason\": \"The score is 1.0 because the \\'actual output\\' aligns perfectly with the information presented in the \\'retrieval context\\', showcasing the health benefits, disease risk reduction, cognitive function improvement, and overall well-being promotion of the Mediterranean diet.\"\\n  }\\n}\\n```\\n## Roadmap\\n\\nWe have an exciting roadmap planned for IndoxJudge:\\n\\n  | Plan                                                                 |\\n| -------------------------------------------------------------------- |\\n | Integration of additional metrics such as Diversity and Coherence.   |\\n| Introduction of a graphical user interface (GUI) for easier evaluation. |\\n  | Expansion of the toolkit to support evaluation in multiple languages. |\\n  | Release of a benchmarking suite for standardizing LLM evaluations.   |\\n\\n## Contributing\\n\\nWe welcome contributions from the community! If you\\'d like to contribute, please fork the repository and create a pull request. For major changes, please open an issue first to discuss what you would like to change.\\n\\n1. Fork the repository\\n2. Create a new branch (`git checkout -b feature-branch`)\\n3. Commit your changes (`git commit -am \\'Add new feature\\'`)\\n4. Push to the branch (`git push origin feature-branch`)\\n5. Create a pull request\\n\\n\\n',\n",
       " '# AnswerRelevancy\\n\\nClass for evaluating the relevancy of language model outputs by analyzing statements, generating verdicts, and calculating relevancy scores.\\n\\n## Initialization\\n\\nThe `AnswerRelevancy` class is initialized with the following parameters:\\n\\n- `query`: The query being evaluated.\\n- `llm_response`: The response generated by the language model.\\n- `model`: The language model to use for evaluation. Defaults to `None`.\\n- `threshold`: The threshold for determining relevancy. Defaults to `0.5`.\\n- `include_reason`: Whether to include reasoning for the relevancy verdicts. Defaults to `True`.\\n- `strict_mode`: Whether to use strict mode, which forces a score of 0 if relevancy is below the threshold. Defaults to `False`.\\n\\n```python\\nclass AnswerRelevancy:\\n    \"\"\"\\n    Class for evaluating the relevancy of language model outputs by analyzing statements,\\n    generating verdicts, and calculating relevancy scores.\\n    \"\"\"\\n    def __init__(self, query: str, llm_response: str, model=None, threshold: float = 0.5, include_reason: bool = True,\\n                 strict_mode: bool = False):\\n        \"\"\"\\n        Initializes the AnswerRelevancy class with the query, LLM response, and evaluation settings.\\n\\n        :param query: The query being evaluated.\\n        :param llm_response: The response generated by the language model.\\n        :param model: The language model to use for evaluation. Defaults to None.\\n        :param threshold: The threshold for determining relevancy. Defaults to 0.5.\\n        :param include_reason: Whether to include reasoning for the relevancy verdicts. Defaults to True.\\n        :param strict_mode: Whether to use strict mode, which forces a score of 0 if relevancy is below the threshold. Defaults to False.\\n        \"\"\"\\n```\\n# Hyperparameters Explanation\\n\\n- **query**: A string containing the query for which relevancy is being evaluated.\\n\\n- **llm_response**: A string containing the response from the language model that needs to be evaluated.\\n\\n- **model**: This is the language model object used for the evaluation process. If not provided, the default model is used.\\n\\n- **threshold**: A float value representing the minimum relevancy score required for a response to be considered relevant. The default value is 0.5.\\n\\n- **include_reason**: A boolean indicating whether the evaluation should include detailed reasons for the relevancy verdict. Default is True.\\n\\n- **strict_mode**: A boolean that, when set to True, ensures that any score below the threshold results in a relevancy score of 0. This is useful for enforcing strict relevancy criteria. Default is False.\\n\\n# Usage Example\\n\\nHere is an example of how to use the `AnswerRelevancy` class:\\n\\n```python\\nimport os\\nfrom dotenv import load_dotenv\\nfrom indoxJudge.models import OpenAi\\nfrom indoxJudge.metrics import AnswerRelevancy\\nfrom indoxJudge.piplines import CustomEvaluator\\n\\nload_dotenv()\\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\\n\\n# Initialize the language model\\nllm = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\\n\\n# Define the query and the response to be evaluated\\nquery = \"What is the capital of France?\"\\nllm_response = \"The capital of France is Paris.\"\\n\\n# Initialize the AnswerRelevancy metric\\nanswer_relevancy_metric = AnswerRelevancy(\\n    query=query, \\n    llm_response=llm_response, \\n    threshold=0.5, \\n    include_reason=True, \\n    strict_mode=False\\n)\\n\\nevaluator = CustomEvaluator(model=llm, metrics=[answer_relevancy_metric])\\nresult = evaluator.judge()\\n```\\n\\n# Hyperparameters Explanation\\n\\n- **query**: A string representing the query for which the contextual relevancy is being evaluated.\\n\\n- **retrieval_context**: A list of strings, each representing a context retrieved in response to the query.\\n\\n',\n",
       " '# BLEU\\n\\nClass for evaluating the similarity between a generated response and one or more expected responses using the BLEU metric, which is based on n-gram overlaps.\\n\\n## Initialization\\n\\nThe `BLEU` class is initialized with the following parameters:\\n\\n- **llm_response**: The response generated by a language model.\\n- **retrieval_context**: The expected response(s) to compare against the actual response.\\n- **n**: The maximum size of the n-grams to use for evaluation.\\n- **remove_repeating_ngrams**: Option to remove repeating n-grams from consideration.\\n\\n```python\\nclass BLEU:\\n    def __init__(\\n        self,\\n        llm_response: str,\\n        retrieval_context: Union[str, List[str]],\\n        n: int = 2,\\n        remove_repeating_ngrams: bool = False,\\n    ):\\n        \"\"\"\\n        Initialize the BLEU evaluator with the desired n-gram size and option to remove repeating n-grams.\\n\\n        Parameters:\\n        llm_response (str): The response generated by a language model.\\n        retrieval_context (Union[str, List[str]]): The expected response(s) to compare against the actual response.\\n        n (int): The maximum size of the n-grams to use for evaluation (default is 2).\\n        remove_repeating_ngrams (bool): Whether to remove repeating n-grams (default is False).\\n        \"\"\"\\n```\\n## Parameters Explanation\\n\\n- **llm_response**: The actual response generated by the language model that needs to be evaluated.\\n\\n- **retrieval_context**: The expected responses for comparison. Can be a single string or a list of strings.\\n\\n- **n**: The maximum n-gram size to use for evaluation. Default is `2`.\\n\\n- **remove_repeating_ngrams**: A boolean flag to indicate whether to remove repeating n-grams from consideration. Default is `False`.\\n## Usage Example\\n\\nHere is an example of how to use the `BLEU` class:\\n\\n```python\\nfrom indoxJudge.metrics import BLEU\\nfrom indoxJudge.piplines import CustomEvaluator\\n\\n# Define a sample response and context\\nllm_response = \"The quick brown fox jumps over the lazy dog.\"\\nretrieval_context = [\\n    \"The fast brown fox leaps over the lazy dog.\",\\n    \"A speedy brown fox jumps over a sleepy dog.\"\\n]\\n\\n# Initialize the BLEU object\\nbleu = BLEU(\\n    llm_response=llm_response,\\n    retrieval_context=retrieval_context,\\n    n=2,\\n    remove_repeating_ngrams=False\\n)\\n\\n# Measure the BLEU score\\nevaluator = CustomEvaluator(model=None, metrics=[bleu])\\nresult = evaluator.judge()\\n```',\n",
       " '# BertScore\\n\\nClass for evaluating the similarity between a generated response and one or more expected responses using embeddings from a pre-trained transformer model.\\n\\n## Initialization\\n\\nThe `BertScore` class is initialized with the following parameters:\\n\\n- **llm_response**: The response generated by a language model.\\n- **retrieval_context**: The expected response(s) to compare against the actual response.\\n- **model_name**: The identifier for the pre-trained transformer model to use for generating embeddings.\\n- **max_length**: The maximum length of input sequences to be processed by the model.\\n\\n```python\\nclass BertScore:\\n    def __init__(\\n        self,\\n        llm_response\\n        retrieval_context,\\n        model_name = \"bert-base-uncased\",\\n        max_length = 1024,\\n    ):\\n        \"\"\"\\n        Initialize the BertScore class to evaluate the similarity between a generated response\\n        and one or more expected responses using a specified pre-trained transformer model.\\n\\n        Parameters:\\n        llm_response (str): The response generated by a language model.\\n        retrieval_context (Union[str, List[str]]): The expected response(s) to compare against the actual response.\\n        model_name (str): The identifier for the pre-trained model to be used for generating embeddings.\\n                          Defaults to \"roberta-base\".\\n        max_length (int): The maximum length of input sequences to be processed by the model. Defaults to 1024.\\n        \"\"\"\\n```\\n## Parameters Explanation\\n\\n- **llm_response**: The actual response generated by the language model that needs to be evaluated.\\n\\n- **retrieval_context**: The expected responses for comparison. Can be a single string or a list of strings.\\n\\n- **model_name**: The name of the pre-trained transformer model used for generating text embeddings. Default is `\"bert-base-uncased\"`.\\n\\n- **max_length**: The maximum length for input sequences that the model will handle. Default is `1024`.\\n\\n## Usage Example\\n\\nHere is an example of how to use the `BertScore` class:\\n\\n```python\\nfrom indoxJudge.metrics import BertScore\\nfrom indoxJudge.piplines import CustomEvaluator\\n\\n# Define a sample response and context\\nllm_response = \"The quick brown fox jumps over the lazy dog.\"\\nretrieval_context = [\\n    \"The fast brown fox leaps over the lazy dog.\",\\n    \"A speedy brown fox jumps over a sleepy dog.\"\\n]\\n\\n# Initialize the BertScore object\\nbert_score = BertScore(\\n    llm_response=llm_response,\\n    retrieval_context=retrieval_context,\\n    max_length=512\\n)\\n\\n# Measure the similarity\\nevaluator = CustomEvaluator(model=None, metrics=[bert_score])\\nresult = evaluator.judge()\\n```',\n",
       " '# Bias\\n\\nClass for evaluating potential bias in language model outputs by analyzing opinions, generating verdicts, and calculating bias scores.\\n\\n## Initialization\\n\\nThe `Bias` class is initialized with the following parameters:\\n\\n- **llm_response**: The response generated by the language model.\\n- **threshold**: The threshold for determining bias. Defaults to `0.5`.\\n- **include_reason**: Whether to include reasoning for the bias verdicts. Defaults to `True`.\\n- **strict_mode**: Whether to use strict mode, which forces a score of 1 if bias exceeds the threshold. Defaults to `False`.\\n\\n```python\\nclass Bias:\\n    \"\"\"\\n    Class for evaluating potential bias in language model outputs by analyzing opinions,\\n    generating verdicts, and calculating bias scores.\\n    \"\"\"\\n    def __init__(self, llm_response, threshold: float = 0.5, include_reason: bool = True, strict_mode: bool = False):\\n        \"\"\"\\n        Initializes the Bias class with the LLM response and evaluation settings.\\n\\n        :param llm_response: The response generated by the language model.\\n        :param threshold: The threshold for determining bias. Defaults to 0.5.\\n        :param include_reason: Whether to include reasoning for the bias verdicts. Defaults to True.\\n        :param strict_mode: Whether to use strict mode, which forces a score of 1 if bias exceeds the threshold. Defaults to False.\\n        \"\"\"\\n  ```\\n# Hyperparameters Explanation\\n\\n- **llm_response**: The response from the language model that is being evaluated for bias.\\n\\n- **threshold**: A float value representing the bias threshold. If the bias score exceeds this threshold, the output may be flagged as biased. The default value is 0.5.\\n\\n- **include_reason**: A boolean that determines whether the evaluation should include reasoning for why a certain bias score was assigned. Default is True.\\n\\n- **strict_mode**: A boolean that, when set to True, forces a score of 1 if the bias exceeds the threshold, regardless of the exact score. This is useful for stringent bias detection. Default is False.\\n\\n# Usage Example\\n\\nHere is an example of how to use the `Bias` class:\\n\\n```python\\nimport os\\nfrom dotenv import load_dotenv\\nfrom indoxJudge.models import OpenAi\\nfrom indoxJudge.metrics import Bias\\nfrom indoxJudge.piplines import CustomEvaluator\\nload_dotenv()\\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\\n\\n# Initialize the language model\\nllm = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\\n\\n# Define the query and the response to be evaluated\\nquery = \"What is the capital of France?\"\\nllm_response = \"The capital of France is Paris.\"\\n\\n# Initialize the evaluation metrics\\nbias_metric = Bias(llm_response=llm_response, threshold=0.5, include_reason=True, strict_mode=False)\\n\\n# Initialize the Evaluator with the model and metrics\\nevaluator = CustomEvaluator(model=llm, metrics=[bias_metric])\\nresult = evaluator.judge()\\n```',\n",
       " '# ContextualRelevancy\\n\\nClass for evaluating the contextual relevancy of retrieval contexts based on a given query using a specified language model.\\n\\n## Initialization\\n\\nThe `ContextualRelevancy` class is initialized with the following parameters:\\n\\n- **query**: The query being evaluated.\\n- **retrieval_context**: A list of contexts retrieved for the query.\\n\\n```python\\nclass ContextualRelevancy:\\n    \"\"\"\\n    Class for evaluating the contextual relevancy of retrieval contexts based on a given query\\n    using a specified language model.\\n    \"\"\"\\n    def __init__(self, query: str, retrieval_context: List[str]):\\n        \"\"\"\\n        Initializes the ContextualRelevancy class with the query and retrieval contexts.\\n\\n        :param query: The query being evaluated.\\n        :param retrieval_context: A list of contexts retrieved for the query.\\n        \"\"\"\\n```\\n# Usage Example\\n\\nHere is an example of how to use the `ContextualRelevancy` class:\\n\\n```python\\nimport os\\nfrom dotenv import load_dotenv\\nfrom indoxJudge.models import OpenAi\\nfrom indoxJudge.piplines import CustomEvaluator\\nfrom indoxJudge.metrics import ContextualRelevancy\\n\\nload_dotenv()\\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\\n\\n# Initialize the language model\\nllm = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\\n\\n# Define the query and the retrieval contexts to be evaluated\\nquery = \"What are the main causes of global warming?\"\\nretrieval_context = [\\n    \"Human activities, such as burning fossil fuels, deforestation, and industrial processes, are major contributors.\",\\n    \"Natural factors, including volcanic eruptions and variations in solar radiation, also play a role.\",\\n    \"The greenhouse effect, driven by the accumulation of greenhouse gases like CO2, is a key mechanism.\"\\n]\\n\\n# Initialize the ContextualRelevancy metric\\ncontextual_relevancy_metric = ContextualRelevancy(query=query, retrieval_context=retrieval_context)\\nevaluator = CustomEvaluator(model=llm, metrics=[contextual_relevancy_metric])\\nresult = evaluator.judge()\\n```',\n",
       " '\\n# Fairness\\n\\nClass for evaluating the fairness of language model outputs by analyzing the fairness score, reasons, and verdicts using a specified language model.\\n\\n## Initialization\\n\\nThe `Fairness` class is initialized with the following parameters:\\n\\n- **input_sentence**: The response generated by the language model.\\n\\n```python\\nclass Fairness:\\n    \"\"\"\\n    Class for evaluating the fairness of language model outputs by analyzing\\n    the fairness score, reasons, and verdicts using a specified language model.\\n    \"\"\"\\n    def __init__(self, input_sentence: str):\\n        \"\"\"\\n        Initializes the Fairness class with the input sentence to be evaluated.\\n\\n        :param input_sentence: The sentence to be evaluated for fairness.\\n        \"\"\"\\n```\\n\\n# Hyperparameters Explanation\\n\\n- **input_sentence**: The response from the language model that needs to be evaluated for fairness.\\n\\n# Usage Example\\n\\nHere is an example of how to use the `Fairness` class:\\n\\n```python\\nfrom indoxJudge.piplines.safetyEvaluator.safety_evaluator import SafetyEvaluator\\n\\n# Define the input sentence to be evaluated\\ninput_sentence = \"Most people believe that teamwork is essential for successful project completion, although some individuals may work better independently.\"\\n\\n# Initialize the safety model with the provided input sentence\\nsafety_model = SafetyEvaluator(model=model, input=input_sentence)\\n\\n# Judge the metrics for fairness\\nmetrics_score, metrics_reasons = safety_model.judge()\\n\\n# Transform and print the metrics\\ntransformed_metrics = safety_model.transform_metrics()\\n\\nprint(\"Metrics Scores:\")\\nprint(json.dumps(metrics_score, indent=4))\\n\\nprint(\"\\\\nMetrics Reasons:\")\\nprint(json.dumps(metrics_reasons, indent=4))\\n\\nprint(\"Transformed Metrics:\", transformed_metrics)\\n```\\n',\n",
       " '# Faithfulness\\n\\nClass for evaluating the faithfulness of language model outputs by analyzing claims, truths, verdicts, and reasons using a specified language model.\\n\\n## Initialization\\n\\nThe `Faithfulness` class is initialized with the following parameters:\\n\\n- **llm_response**: The response generated by the language model.\\n- **retrieval_context**: The context used for retrieval during evaluation.\\n\\n```python\\nclass Faithfulness:\\n    \"\"\"\\n    Class for evaluating the faithfulness of language model outputs by analyzing\\n    claims, truths, verdicts, and reasons using a specified language model.\\n    \"\"\"\\n    def __init__(self, llm_response, retrieval_context):\\n        \"\"\"\\n        Initializes the Faithfulness class with the LLM response and retrieval context.\\n\\n        :param llm_response: The response generated by the language model.\\n        :param retrieval_context: The context used for retrieval during evaluation.\\n        \"\"\"\\n```\\n# Hyperparameters Explanation\\n\\n- **llm_response**: The response from the language model that needs to be evaluated for faithfulness.\\n\\n- **retrieval_context**: The context or reference information used to verify the truthfulness and accuracy of the language model\\'s output.\\n\\n# Usage Example\\n\\nHere is an example of how to use the `Faithfulness` class:\\n\\n```python\\nimport os\\nfrom dotenv import load_dotenv\\nfrom indoxJudge.models import OpenAi\\nfrom indoxJudge.metrics import Faithfulness\\nfrom indoxJudge.piplines import CustomEvaluator\\n\\nload_dotenv()\\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\\n\\n# Initialize the language model\\nllm = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\\n\\n# Define the LLM response and retrieval context to be evaluated\\nllm_response = \"Paris is the capital of Germany.\"\\nretrieval_context = \"Paris is the capital of France.\"\\n\\n# Initialize the Faithfulness metric\\nfaithfulness_metric = Faithfulness(llm_response=llm_response, retrieval_context=retrieval_context)\\n\\n# Create an evaluator with the Faithfulness metric\\nevaluator = CustomEvaluator(model=llm, metrics=[faithfulness_metric])\\nresult = evaluator.judge()\\n```',\n",
       " '# GEval\\n\\nClass for evaluating various aspects of language model outputs, including retrieval quality, integration, coherence, relevance, accuracy, fluency, comprehensiveness, and contextuality.\\n\\n## Initialization\\n\\nThe `GEval` class is initialized with the following parameters:\\n\\n- **parameters**: The parameters or aspects to evaluate (e.g., \\'summary\\', \\'dialogue\\').\\n- **query**: The original query or input text.\\n- **llm_response**: The response generated by the language model.\\n- **ground_truth**: The expected or correct output.\\n- **context**: Additional context relevant to the query.\\n- **retrieval_context**: The context from which information was retrieved.\\n\\n```python\\nclass GEval:\\n    def __init__(self, parameters, query, llm_response, ground_truth, context, retrieval_context):\\n        \"\"\"\\n        Initialize the GEval class with necessary inputs for evaluation.\\n\\n        Parameters:\\n        parameters (str): The parameters or aspects to evaluate (e.g., \\'summary\\', \\'dialogue\\').\\n        query (str): The original query or input text.\\n        llm_response (str): The response generated by the language model.\\n        ground_truth (str): The expected or correct output.\\n        context (str): Additional context relevant to the query.\\n        retrieval_context (str): The context from which information was retrieved.\\n        \"\"\"\\n\\n        self.criteria = \"\"\"\\n        1. Retrieval Quality: The retrieved documents or snippets should be relevant and accurate.\\n        2. Integration: The retrieved information should be well integrated into the generated response.\\n        3. Coherence: The text should be logically structured and easy to follow.\\n        4. Relevance: The text should be relevant to the main topic and cover all key points.\\n        5. Accuracy: The text should be factually accurate and consistent with the source material.\\n        6. Fluency: The text should be easy to read and free from grammatical errors.\\n        7. Comprehensiveness: The text should cover all key points and provide a thorough response.\\n        8. Contextuality: The response should fit well within the context of the query.\\n        \"\"\"\\n```\\n\\n# Parameters Explanation\\n\\n- **parameters**: Specifies the aspects to evaluate, such as \\'summary\\', \\'dialogue\\', etc.\\n- **query**: The initial question or input provided for generating the response.\\n- **llm_response**: The actual response generated by the language model that needs evaluation.\\n- **ground_truth**: The correct or expected answer for comparison.\\n- **context**: Any additional background or context related to the query.\\n- **retrieval_context**: Information retrieved that helps in generating the response, serving as a reference.\\n\\n# Evaluation Criteria\\n\\nThe evaluation is based on several criteria, which include:\\n\\n- **Retrieval Quality**: The relevance and accuracy of the retrieved documents or snippets.\\n- **Integration**: How well the retrieved information is integrated into the generated response.\\n- **Coherence**: The logical structure and ease of following the text.\\n- **Relevance**: The pertinence of the text to the main topic and completeness in covering key points.\\n- **Accuracy**: The factual correctness and consistency of the text with the source material.\\n- **Fluency**: The readability and grammatical correctness of the text.\\n- **Comprehensiveness**: The thoroughness of the text in covering all key points.\\n- **Contextuality**: The appropriateness of the response within the context of the query.\\n\\n# Usage Example\\n\\nHere is an example of how to use the `GEval` class:\\n\\n```python\\nimport os\\nfrom dotenv import load_dotenv\\nfrom indoxJudge.models import OpenAi\\nfrom indoxJudge.metrics import GEval\\nfrom indoxJudge.piplines import CustomEvaluator\\n\\nload_dotenv()\\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\\n\\n# Initialize the language model\\nllm = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\\n\\n# Define the necessary inputs for evaluation\\nquery = \"What are the main benefits of a plant-based diet?\"\\nllm_response = \"A plant-based diet can improve heart health, aid in weight loss, and reduce the risk of chronic diseases.\"\\nground_truth = \"A plant-based diet is known for its benefits like improving cardiovascular health, aiding weight loss, and reducing the risk of chronic diseases such as diabetes and cancer.\"\\ncontext = \"Plant-based diets are associated with lower levels of cholesterol and blood pressure.\"\\nretrieval_context = \"Studies have shown that plant-based diets can lower the risk of heart disease and obesity.\"\\n\\n# Initialize the GEval metric\\ngeval_metric = GEval(\\n    parameters=\"summary\",\\n    query=query,\\n    llm_response=llm_response,\\n    ground_truth=ground_truth,\\n    context=context,\\n    retrieval_context=retrieval_context\\n)\\n\\n# Create an evaluator with the GEval metric\\nevaluator = CustomEvaluator(model=llm, metrics=[geval_metric])\\nresult = evaluator.judge()\\n```',\n",
       " '# Gruen\\n\\nClass for evaluating the quality of generated text using various metrics, including grammaticality, redundancy, and focus.\\n\\n## Initialization\\n\\nThe `Gruen` class is initialized with the following parameters:\\n\\n- **candidates**: The candidate text(s) to evaluate.\\n\\n```python\\nclass Gruen:\\n    def __init__(self, candidates: Union[str, List[str]], use_spacy: bool = True, use_nltk: bool = True):\\n        \"\"\"\\n        Initialize the TextEvaluator with candidate texts and options to use spacy and nltk.\\n\\n        Parameters:\\n        candidates (Union[str, List[str]]): The candidate text(s) to evaluate.\\n        \"\"\"\\n        if isinstance(candidates, str):\\n            candidates = [candidates]\\n        self.candidates = candidates\\n        self.stop_words = set(stopwords.words(\\'english\\'))\\n        self.lemmatizer = WordNetLemmatizer()\\n```\\n## Parameters Explanation\\n\\n- **candidates**: The actual texts to be evaluated. Can be a single string or a list of strings.\\n\\n## Usage Example\\n\\nHere is an example of how to use the `Gruen` class:\\n\\n```python\\nfrom indoxJudge.metrics import Gruen\\nfrom indoxJudge.piplines import CustomEvaluator\\n\\n# Define sample candidate texts\\ncandidates = [\\n    \"The quick brown fox jumps over the lazy dog.\",\\n    \"A fast brown fox leaps over a sleepy dog.\"\\n]\\n\\n# Initialize the Gruen object\\ngruen = Gruen(\\n    candidates=candidates,\\n)\\n\\n# Calculate the GRUEN scores\\nevaluator = CustomEvaluator(model=None, metrics=[gruen])\\nresult = evaluator.judge()\\n```',\n",
       " '# Hallucination\\n\\nClass for evaluating hallucinations in language model outputs by analyzing the generated responses, generating verdicts, and calculating hallucination scores.\\n\\n## Initialization\\n\\nThe `Hallucination` class is initialized with the following parameters:\\n\\n- **llm_response**: The response generated by the language model.\\n- **retrieval_context**: The context from which information was retrieved for comparison.\\n- **threshold**: The threshold for determining hallucinations. Defaults to `0.5`.\\n- **include_reason**: Whether to include reasoning for the hallucination verdicts. Defaults to `True`.\\n- **strict_mode**: Whether to use strict mode, which forces a score of 1 if hallucination exceeds the threshold. Defaults to `False`.\\n\\n```python\\nclass Hallucination:\\n    \"\"\"\\n    Class for evaluating hallucinations in language model outputs by analyzing the generated responses,\\n    generating verdicts, and calculating hallucination scores.\\n    \"\"\"\\n    def __init__(self, llm_response: str, retrieval_context: str, threshold: float = 0.5, include_reason: bool = True,\\n                 strict_mode: bool = False):\\n        \"\"\"\\n        Initializes the Hallucination class with the LLM response, retrieval context, and evaluation settings.\\n\\n        Parameters:\\n        llm_response (str): The response generated by the language model.\\n        retrieval_context (str): The context from which information was retrieved for comparison.\\n        threshold (float): The threshold for determining hallucinations. Defaults to 0.5.\\n        include_reason (bool): Whether to include reasoning for the hallucination verdicts. Defaults to True.\\n        strict_mode (bool): Whether to use strict mode, which forces a score of 1 if hallucination exceeds the threshold. Defaults to False.\\n        \"\"\"\\n```\\n# Hyperparameters Explanation\\n\\n- **llm_response**: A string containing the response from the language model that is being evaluated for hallucinations.\\n\\n- **retrieval_context**: A string containing the context or reference information used to verify the accuracy of the language model\\'s output.\\n\\n- **threshold**: A float value representing the hallucination threshold. If the hallucination score exceeds this threshold, the output may be flagged as a hallucination. The default value is 0.5.\\n\\n- **include_reason**: A boolean indicating whether the evaluation should include detailed reasons for the hallucination verdict. Default is True.\\n\\n- **strict_mode**: A boolean that, when set to True, forces a score of 1 if the hallucination exceeds the threshold, regardless of the exact score. This is useful for stringent hallucination detection. Default is False.\\n\\n# Usage Example\\n\\nHere is an example of how to use the `Hallucination` class:\\n\\n```python\\nimport os\\nfrom dotenv import load_dotenv\\nfrom indoxJudge.models import OpenAi\\nfrom indoxJudge.metrics import Hallucination\\nfrom indoxJudge.piplines import CustomEvaluator\\n\\nload_dotenv()\\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\\n\\n# Initialize the language model\\nllm = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\\n\\n# Define the response and retrieval context to be evaluated\\nllm_response = \"The Eiffel Tower is located in Berlin.\"\\nretrieval_context = \"The Eiffel Tower is located in Paris, France.\"\\n\\n# Initialize the Hallucination evaluation metric\\nhallucination_metric = Hallucination(\\n    llm_response=llm_response, \\n    retrieval_context=retrieval_context, \\n    threshold=0.5, \\n    include_reason=True, \\n    strict_mode=False\\n)\\n\\n# Create an evaluator with the Hallucination metric\\nevaluator = CustomEvaluator(model=llm, metrics=[hallucination_metric])\\nresult = evaluator.judge()\\n```\\n',\n",
       " '\\n---\\n\\n# Harmfulness\\n\\nClass for evaluating the potential harmfulness of language model outputs by analyzing the input sentence and generating harmfulness scores, reasons, and verdicts using a specified language model.\\n\\n## Initialization\\n\\nThe `Harmfulness` class is initialized with the following parameters:\\n\\n- **input_sentence**:  The response generated by the language model.\\n\\n```python\\nclass Harmfulness:\\n    \"\"\"\\n    Class for evaluating the potential harmfulness of language model outputs by analyzing\\n    the input sentence and generating harmfulness scores, reasons, and verdicts using a specified language model.\\n    \"\"\"\\n    def __init__(self, input_sentence: str):\\n        \"\"\"\\n        Initializes the Harmfulness class with the input sentence.\\n\\n        :param input_sentence: The sentence or text that is evaluated for harmfulness.\\n        \"\"\"\\n```\\n\\n# Hyperparameters Explanation\\n\\n- **input_sentence**: The response from the language model that needs to be evaluated for harmfulness.\\n\\n## Usage Example\\n\\nHere is an example of how to use the `Harmfulness` class:\\n\\n```python\\nfrom indoxJudge.piplines.safetyEvaluator.safety_evaluator import SafetyEvaluatorfrom indoxJudge.piplines.safetyEvaluator.safety_evaluator import SafetyEvaluator\\ninput_sentence = \"Most people believe that teamwork is essential for successful project completion, although some individuals may work better independently.\"\\n\\n\\nsafety_model = SafetyEvaluator(model=model, input=input_sentence)\\n\\nmetrics_score, metrics_reasons = safety_model.judge()\\n\\ntransformed_metrics = safety_model.transform_metrics()\\n# Print the evaluation results\\nprint(\"Metrics Scores:\")\\nprint(json.dumps(metrics_score, indent=4))\\n\\nprint(\"\\\\nMetrics Reasons:\")\\nprint(json.dumps(metrics_reasons, indent=4))\\nprint(\"Transformed Metrics:\", transformed_metrics)\\n```\\n\\n---\\n\\nFeel free to adjust any part of the documentation to better fit your needs or to provide additional details if necessary!\\n',\n",
       " '# KnowledgeRetention\\n\\nClass for evaluating the retention of knowledge in language model outputs by analyzing the continuity of knowledge across multiple messages, generating verdicts, and calculating retention scores.\\n\\n## Initialization\\n\\nThe `KnowledgeRetention` class is initialized with the following parameters:\\n\\n- **messages**: A list of messages containing queries and LLM responses.\\n- **threshold**: The threshold for determining successful knowledge retention. Defaults to `0.5`.\\n- **include_reason**: Whether to include reasoning for the knowledge retention verdicts. Defaults to `True`.\\n- **strict_mode**: Whether to use strict mode, which forces a score of 0 if retention is below the threshold. Defaults to `False`.\\n\\n```python\\nclass KnowledgeRetention:\\n    \"\"\"\\n    Class for evaluating the retention of knowledge in language model outputs by analyzing the continuity of knowledge\\n    across multiple messages, generating verdicts, and calculating retention scores.\\n    \"\"\"\\n    def __init__(self, messages: List[Dict[str, str]], threshold: float = 0.5, include_reason: bool = True, strict_mode: bool = False):\\n        \"\"\"\\n        Initializes the KnowledgeRetention class with the messages, threshold, and evaluation settings.\\n\\n        Parameters:\\n        messages (List[Dict[str, str]]): A list of messages containing queries and LLM responses.\\n        threshold (float): The threshold for determining successful knowledge retention. Defaults to 0.5.\\n        include_reason (bool): Whether to include reasoning for the knowledge retention verdicts. Defaults to True.\\n        strict_mode (bool): Whether to use strict mode, which forces a score of 0 if retention is below the threshold. Defaults to False.\\n        \"\"\"\\n```\\n# Hyperparameters Explanation\\n\\n- **messages**: A list of dictionaries, where each dictionary contains a query and the corresponding `llm_response`. This allows for evaluation of how well the language model retains knowledge across multiple interactions.\\n\\n- **threshold**: A float value representing the minimum retention score required for knowledge retention to be considered successful. The default value is 0.5.\\n\\n- **include_reason**: A boolean that indicates whether to provide detailed reasoning for the retention score verdicts. Default is True.\\n\\n- **strict_mode**: A boolean that, when set to True, forces a score of 0 if the retention score is below the threshold. This is useful for strict evaluation criteria. Default is False.\\n\\n# Usage Example\\n\\nHere is an example of how to use the `KnowledgeRetention` class:\\n\\n```python\\nimport os\\nfrom dotenv import load_dotenv\\nfrom indoxJudge.models import OpenAi\\nfrom indoxJudge.metrics import KnowledgeRetention\\nfrom indoxJudge.piplines import CustomEvaluator\\n\\nload_dotenv()\\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\\n\\n# Initialize the language model\\nllm = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\\n\\n# Define the messages containing queries and LLM responses\\nmessages = [\\n    {\"query\": \"What is the capital of France?\", \"llm_response\": \"The capital of France is Paris.\"},\\n    {\"query\": \"Where is the Eiffel Tower located?\", \"llm_response\": \"The Eiffel Tower is located in Berlin.\"}\\n]\\n\\n# Initialize the KnowledgeRetention evaluation metric\\nknowledge_retention_metric = KnowledgeRetention(\\n    messages=messages, \\n    threshold=0.5, \\n    include_reason=True, \\n    strict_mode=False\\n)\\n\\n# Create an evaluator with the KnowledgeRetention metric\\nevaluator = CustomEvaluator(model=llm, metrics=[knowledge_retention_metric])\\nresult = evaluator.judge()\\n```\\n',\n",
       " '# METEOR\\n\\nClass for evaluating the similarity between a generated response and one or more reference contexts using the METEOR metric, which considers precision, recall, and fragmentation.\\n\\n## Initialization\\n\\nThe `METEOR` class is initialized with the following parameters:\\n\\n- **llm_response**: The response generated by a language model.\\n- **retrieval_context**: The reference context(s) to compare against the actual response.\\n\\n```python\\nclass METEOR:\\n    def __init__(self, llm_response: str, retrieval_context: Union[str, List[str]]):\\n        \"\"\"\\n        Initialize the METEOR evaluator.\\n\\n        Parameters:\\n        llm_response (str): The response generated by the Language Model.\\n        retrieval_context (Union[str, List[str]]): The reference context(s) against which the response is evaluated.\\n        \"\"\"\\n        self.llm_response = llm_response\\n        self.retrieval_context = retrieval_context\\n```\\n## Parameters Explanation\\n\\n- **llm_response**: The actual response generated by the language model that needs to be evaluated.\\n\\n- **retrieval_context**: The reference contexts for comparison. Can be a single string or a list of strings.\\n\\n## Usage Example\\n\\nHere is an example of how to use the `METEOR` class:\\n\\n```python\\nfrom indoxJudge.metrics import METEOR\\nfrom indoxJudge.piplines import CustomEvaluator\\n\\n# Define a sample response and context\\nllm_response = \"The quick brown fox jumps over the lazy dog.\"\\nretrieval_context = [\\n    \"The fast brown fox leaps over the lazy dog.\",\\n    \"A speedy brown fox jumps over a sleepy dog.\"\\n]\\n\\n# Initialize the METEOR object\\nmeteor = METEOR(\\n    llm_response=llm_response,\\n    retrieval_context=retrieval_context\\n)\\n\\n# Measure the METEOR score\\nevaluator = CustomEvaluator(model=None, metrics[meteor])\\nresult = evaluator.judge()\\n```',\n",
       " '# Machine Ethics\\n\\nClass for evaluating the ethical implications of language model outputs by analyzing input sentences and generating verdicts, reasons, and ethical scores using a specified language model.\\n\\n## Initialization\\n\\nThe `MachineEthics` class is initialized with the following parameters:\\n\\n- **input_sentence**: The response generated by the language model.\\n\\n```python\\nclass MachineEthics:\\n    \"\"\"\\n    Class for evaluating the ethical implications of language model outputs by analyzing input sentences\\n    and generating verdicts, reasons, and ethical scores using a specified language model.\\n    \"\"\"\\n    def __init__(self, input_sentence: str):\\n        \"\"\"\\n        Initializes the MachineEthics class with the input sentence.\\n\\n        :param input_sentence: The sentence or text that needs to be evaluated for ethical considerations.\\n        \"\"\"\\n```\\n\\n## Hyperparameters Explanation\\n\\n- **input_sentence**: The response from the language model that needs to be evaluated for machineEthics.\\n\\n\\n\\n## Usage Example\\n\\nHere is an example of how to use the `MachineEthics` class:\\n\\n```python\\nfrom indoxJudge.piplines.safetyEvaluator.safety_evaluator import SafetyEvaluator\\n\\ninput_sentence = \"Most people believe that teamwork is essential for successful project completion, although some individuals may work better independently.\"\\n\\n# Initialize the safety evaluator with the model and input sentence\\nsafety_model = SafetyEvaluator(model=model, input=input_sentence)\\n\\n# Evaluate the ethics of the input sentence\\nmetrics_score, metrics_reasons = safety_model.judge()\\n\\n# Transform the metrics\\ntransformed_metrics = safety_model.transform_metrics()\\n\\n# Print the evaluation results\\nprint(\"Metrics Scores:\")\\nprint(json.dumps(metrics_score, indent=4))\\n\\nprint(\"\\\\nMetrics Reasons:\")\\nprint(json.dumps(metrics_reasons, indent=4))\\nprint(\"Transformed Metrics:\", transformed_metrics)\\n```\\n\\n',\n",
       " '# Misinformation\\n\\nClass for evaluating the presence of misinformation in an input sentence by analyzing the sentence using a specified language model. It generates a verdict and reasons based on the input sentence, calculating a misinformation score.\\n\\n## Initialization\\n\\nThe Misinformation class is initialized with the following parameters:\\n\\n- **input_sentence**: The response generated by the language model.\\n\\n```python\\nclass Misinformation:\\n    \"\"\"\\n    Class for evaluating the presence of misinformation in an input sentence by analyzing\\n    the sentence using a specified language model. It generates a verdict and reasons based on\\n    the input sentence, calculating a misinformation score.\\n    \"\"\"\\n    def __init__(self, input_sentence: str):\\n        \"\"\"\\n        Initializes the Misinformation class with the input sentence.\\n\\n        :param input_sentence: The sentence that needs to be evaluated for misinformation.\\n        \"\"\"\\n```\\n\\n# Hyperparameters Explanation\\n\\n- **input_sentence**: The response from the language model that needs to be evaluated for misinformation.\\n\\n# Usage Example\\n\\nHere is an example of how to use the Misinformation class:\\n\\n```python\\nimport json\\nfrom indoxJudge.piplines.safetyEvaluator.safety_evaluator import SafetyEvaluator\\n\\ninput_sentence = \"Most people believe that teamwork is essential for successful project completion, although some individuals may work better independently.\"\\n\\n# Initialize the safety evaluator with the model and input sentence\\nsafety_model = SafetyEvaluator(model=model, input=input_sentence)\\n\\n# Evaluate the input sentence\\nmetrics_score, metrics_reasons = safety_model.judge()\\n\\n# Transform the evaluation metrics\\ntransformed_metrics = safety_model.transform_metrics()\\n\\n# Print the evaluation results\\nprint(\"Metrics Scores:\")\\nprint(json.dumps(metrics_score, indent=4))\\n\\nprint(\"\\\\nMetrics Reasons:\")\\nprint(json.dumps(metrics_reasons, indent=4))\\nprint(\"Transformed Metrics:\", transformed_metrics)\\n```\\n',\n",
       " '# Privacy\\n\\nClass for evaluating privacy-related concerns in language model outputs by analyzing input sentences, generating reasons, and determining verdicts using a specified model.\\n\\n## Initialization\\n\\nThe Privacy class is initialized with the following parameters:\\n\\n- **input_sentence**: The response generated by the language model.\\n\\n```python\\nclass Privacy:\\n    \"\"\"\\n    Class for evaluating privacy-related concerns in language model outputs by analyzing\\n    input sentences, generating reasons, and determining verdicts using a specified model.\\n    \"\"\"\\n    def __init__(self, input_sentence: str):\\n        \"\"\"\\n        Initializes the Privacy class with the input sentence.\\n\\n        :param input_sentence: The sentence that needs to be evaluated for privacy concerns.\\n        \"\"\"\\n```\\n\\n## Hyperparameters Explanation\\n\\n- **input_sentence**: The response from the language model that needs to be evaluated for privacy.\\n\\n## Usage Example\\n\\nHere is an example of how to use the Privacy class:\\n\\n```python\\nfrom indoxJudge.piplines.safetyEvaluator.safety_evaluator import SafetyEvaluator\\nimport json\\n\\ninput_sentence = \"Most people believe that teamwork is essential for successful project completion, although some individuals may work better independently.\"\\n\\n# Initialize the safety model\\nsafety_model = SafetyEvaluator(model=model, input=input_sentence)\\n\\n# Evaluate the privacy metrics\\nmetrics_score, metrics_reasons = safety_model.judge()\\n\\n# Transform the metrics for output\\ntransformed_metrics = safety_model.transform_metrics()\\n\\n# Print the evaluation results\\nprint(\"Metrics Scores:\")\\nprint(json.dumps(metrics_score, indent=4))\\n\\nprint(\"\\\\nMetrics Reasons:\")\\nprint(json.dumps(metrics_reasons, indent=4))\\nprint(\"Transformed Metrics:\", transformed_metrics)\\n```\\n',\n",
       " '# Rouge\\n\\nClass for evaluating the similarity between a generated response and one or more expected responses using the ROUGE metric, which considers n-gram overlaps for recall and precision.\\n\\n## Initialization\\n\\nThe `Rouge` class is initialized with the following parameters:\\n\\n- **llm_response**: The response generated by a language model.\\n- **retrieval_context**: The expected response(s) to compare against the actual response.\\n\\n```python\\nclass Rouge:\\n    def __init__(\\n        self, llm_response: str, retrieval_context: Union[str, List[str]], n: int = 1\\n    ):\\n        \"\"\"\\n        Initialize the Rouge evaluator with the desired n-gram size.\\n\\n        Parameters:\\n        llm_response (str): The response generated by a language model.\\n        retrieval_context (Union[str, List[str]]): The expected response(s) to compare against the actual response.\\n        n (int): The size of the n-grams to use for evaluation (e.g., 1 for unigrams, 2 for bigrams, etc.).\\n        \"\"\"\\n```\\n## Parameters Explanation\\n\\n- **llm_response**: The actual response generated by the language model that needs to be evaluated.\\n\\n- **retrieval_context**: The expected responses for comparison. Can be a single string or a list of strings.\\n\\n\\n## Usage Example\\n\\nHere is an example of how to use the `Rouge` class:\\n\\n```python\\nfrom indoxJudge.metrics import Rouge\\nfrom indoxJudge.piplines import CustomEvaluator\\n\\n# Define a sample response and context\"\\nllm_response = \"The quick brown fox jumps over the lazy dog.\"\\n\\nretrieval_context = [\\n    \"The fast brown fox leaps over the lazy dog.\",\\n    \"A speedy brown fox jumps over a sleepy dog.\"\\n]\\n\\n# Initialize the Rouge object\\nrouge = Rouge(\\n    llm_response=llm_response,\\n    retrieval_context=retrieval_context,\\n)\\n\\n# Measure the ROUGE score\\nresult = rouge.measure()\\n```',\n",
       " '# StereotypeBias\\n\\nClass for evaluating the stereotype and bias in language model outputs by analyzing the input sentence, generating reasons, and providing verdicts using a specified language model.\\n\\n## Initialization\\n\\nThe StereotypeBias class is initialized with the following parameters:\\n\\n- **input_sentence**: The response generated by the language model.\\n\\n```python\\nclass StereotypeBias:\\n    \"\"\"\\n    Class for evaluating the stereotype and bias in language model outputs by analyzing\\n    the input sentence, generating reasons, and providing verdicts using a specified language model.\\n    \"\"\"\\n    def __init__(self, input_sentence: str):\\n        \"\"\"\\n        Initializes the StereotypeBias class with the input sentence.\\n\\n        :param input_sentence: The sentence or text input that needs to be evaluated for stereotype and bias.\\n        \"\"\"\\n```\\n\\n## Hyperparameters Explanation\\n\\n- **input_sentence**: The response from the language model that needs to be evaluated for stereotype and bias.\\n\\n## Usage Example\\n\\nHere is an example of how to use the StereotypeBias class:\\n\\n```python\\nimport json\\nfrom indoxJudge.piplines.safetyEvaluator.safety_evaluator import SafetyEvaluator\\n\\n# Input sentence to evaluate\\ninput_sentence = \"Most people believe that teamwork is essential for successful project completion, although some individuals may work better independently.\"\\n\\n# Initialize the safety model with the specified model and input sentence\\nsafety_model = SafetyEvaluator(model=model, input=input_sentence)\\n\\n# Evaluate the metrics and get the score and reasons\\nmetrics_score, metrics_reasons = safety_model.judge()\\n\\n# Transform the metrics for further analysis\\ntransformed_metrics = safety_model.transform_metrics()\\n\\n# Print the evaluation results\\nprint(\"Metrics Scores:\")\\nprint(json.dumps(metrics_score, indent=4))\\n\\nprint(\"\\\\nMetrics Reasons:\")\\nprint(json.dumps(metrics_reasons, indent=4))\\nprint(\"Transformed Metrics:\", transformed_metrics)\\n```\\n',\n",
       " '# Toxicity\\n\\nClass for evaluating toxicity in language model outputs by analyzing opinions, generating verdicts, and calculating toxicity scores.\\n\\n## Initialization\\n\\nThe `Toxicity` class is initialized with the following parameters:\\n\\n- **messages**: A list of messages containing queries and LLM responses.\\n- **threshold**: The threshold for determining toxicity. Defaults to `0.5`.\\n- **include_reason**: Whether to include reasoning for the toxicity verdicts. Defaults to `True`.\\n- **strict_mode**: Whether to use strict mode, which forces a score of 1 if toxicity exceeds the threshold. Defaults to `False`.\\n\\n```python\\nclass Toxicity:\\n    \"\"\"\\n    Class for evaluating toxicity in language model outputs by analyzing opinions,\\n    generating verdicts, and calculating toxicity scores.\\n    \"\"\"\\n    def __init__(self, messages: List[Dict[str, str]],\\n                 threshold: float = 0.5,\\n                 include_reason: bool = True,\\n                 strict_mode: bool = False):\\n        \"\"\"\\n        Initializes the Toxicity class with the messages, threshold, and evaluation settings.\\n\\n        Args:\\n            messages (List[Dict[str, str]]): A list of messages containing queries and LLM responses.\\n            threshold (float): The threshold for determining toxicity. Defaults to 0.5.\\n            include_reason (bool): Whether to include reasoning for the toxicity verdicts. Defaults to True.\\n            strict_mode (bool): Whether to use strict mode, which forces a score of 1 if toxicity exceeds the threshold. Defaults to False.\\n        \"\"\"\\n```\\n# Hyperparameters Explanation\\n\\n- **messages**: A list of dictionaries, where each dictionary contains a query and the corresponding `llm_response`. This is used to evaluate the presence of toxic content in the responses.\\n\\n- **threshold**: A float value representing the toxicity threshold. If the toxicity score exceeds this threshold, the output may be flagged as toxic. The default value is 0.5.\\n\\n- **include_reason**: A boolean indicating whether the evaluation should include detailed reasons for the toxicity verdict. Default is True.\\n\\n- **strict_mode**: A boolean that, when set to True, forces a score of 1 if the toxicity exceeds the threshold, regardless of the exact score. This is useful for stringent toxicity detection. Default is False.\\n\\n# Usage Example\\n\\nHere is an example of how to use the `Toxicity` class:\\n\\n```python\\nimport os\\nfrom dotenv import load_dotenv\\nfrom indoxJudge.models import OpenAi\\nfrom indoxJudge.metrics import Toxicity\\nfrom indoxJudge.piplines import CustomEvaluator\\n\\nload_dotenv()\\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\\n\\n# Initialize the language model\\nllm = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\\n\\n# Define the messages containing queries and LLM responses\\nmessages = [\\n    {\"query\": \"What do you think about different cultures?\", \"llm_response\": \"Some cultures are better than others.\"},\\n    {\"query\": \"What is your opinion on the best country to live in?\", \"llm_response\": \"Only certain countries are worth living in.\"}\\n]\\n\\n# Initialize the Toxicity evaluation metric\\ntoxicity_metric = Toxicity(\\n    messages=messages, \\n    threshold=0.5, \\n    include_reason=True, \\n    strict_mode=False\\n)\\n\\n# Create an evaluator with the Toxicity metric\\nevaluator = CustomEvaluator(model=llm, metrics=[toxicity_metric])\\nresult = evaluator.judge()\\n```\\n',\n",
       " '# CustomEvaluator\\n\\n## Overview\\n\\nThe `CustomEvaluator` class is designed to evaluate various aspects of language model outputs using a range of metrics. It\\nsupports metrics such as Faithfulness, Answer Relevancy, Bias, Contextual Relevancy, GEval, Hallucination, Knowledge\\nRetention, Toxicity, BertScore, BLEU, Rouge, and METEOR. This class is ideal for assessing different dimensions of model\\nperformance and ensuring comprehensive evaluation.\\n\\n## Initialization\\n\\nThe `CustomEvaluator` class is initialized with two main components:\\n\\n- **Model**: The language model to be evaluated.\\n- **Metrics**: A list of metric instances used to evaluate the model.\\n\\n### Example\\n\\n```python\\nclass CustomEvaluator:\\n    def __init__(self, model, metrics: List):\\n        \"\"\"\\n        Initializes the Evaluator with a language model and a list of metrics.\\n\\n        Args:\\n            model: The language model to be evaluated.\\n            metrics (List): A list of metric instances to evaluate the model.\\n        \"\"\"\\n```\\n\\n## Setting the Model for Metrics\\n\\nThe `set_model_for_metrics` method ensures that the language model is properly set for each metric that requires it.\\nThis step is crucial for metrics that need direct access to the model for evaluation purposes.\\n\\n### Example\\n\\n```python\\ndef set_model_for_metrics(self):\\n    \"\"\"\\n    Sets the language model for each metric that requires it.\\n    \"\"\"\\n```\\n\\nIn this method, the `Evaluator` class iterates through each metric in the `metrics` list. If a metric has a `set_model`\\nmethod, it is called with the `model` attribute, ensuring that each metric can access the necessary language model.\\n\\n## Evaluation Process\\n\\nThe `evaluate` method performs the evaluation using the provided metrics. It iterates through each metric, performs the\\nevaluation, and stores the results in a dictionary. The method includes error handling to ensure that any issues\\nencountered during evaluation are logged.\\n\\n### Example\\n\\n```python\\ndef judge(self):\\n    \"\"\"\\n    Evaluates the language model using the provided metrics and returns the results.\\n\\n    Returns:\\n        dict: A dictionary containing the evaluation results for each metric.\\n    \"\"\"\\n    results = {}\\n    for metric in self.metrics:\\n        metric_name = metric.__class__.__name__\\n        try:\\n            logger.info(f\"Evaluating metric: {metric_name}\")\\n            # Example for Faithfulness metric\\n            if isinstance(metric, Faithfulness):\\n                claims = metric.evaluate_claims()\\n                truths = metric.evaluate_truths()\\n                verdicts = metric.evaluate_verdicts(claims.claims)\\n                reason = metric.evaluate_reason(verdicts, truths.truths)\\n                results[\\'faithfulness\\'] = {\\n                    \\'claims\\': claims.claims,\\n                    \\'truths\\': truths.truths,\\n                    \\'verdicts\\': [verdict.__dict__ for verdict in verdicts.verdicts],\\n                    \\'reason\\': reason.reason\\n                }\\n            # Additional metrics (Answer Relevancy, Bias, etc.) follow a similar pattern.\\n            logger.info(f\"Completed evaluation for metric: {metric_name}\")\\n        except Exception as e:\\n            logger.error(f\"Error evaluating metric {metric_name}: {str(e)}\")\\n    return results\\n```\\n\\nIn this example, the Evaluator class method evaluate initializes an empty dictionary called results. It iterates through\\neach metric in the metrics list, retrieves the metric\\'s name, and logs the start of the evaluation process. For each\\nmetric, the method checks its type and calls the appropriate evaluation method. For instance, the Faithfulness metric\\ninvolves evaluating claims, truths, and verdicts, which are then stored in the results dictionary under the \\'\\nfaithfulness\\' key.\\n\\nIf any exceptions are encountered during the evaluation process, they are caught and logged as errors, and the\\nevaluation continues with the next metric. The final results are returned as a dictionary containing the outcomes of the\\nevaluations for each metric.',\n",
       " '# LLMComparison\\n\\n## Overview\\n\\nThe `LLMComparison` class is designed to facilitate the comparison of multiple language models based on their evaluation metrics. This class allows for visual representation of the performance of different models, making it easier to analyze and compare their strengths and weaknesses.\\n\\n## Initialization\\n\\nThe `LLMComparison` class is initialized with a list of models, where each model is represented by a dictionary containing its name, overall score, and various evaluation metrics.\\n\\n### Example\\n\\n```python\\nclass LLMComparison:\\n    def __init__(self, models):\\n        \"\"\"\\n        Initializes the LLMComparison with a list of models.\\n\\n        Args:\\n            models (list): A list of dictionaries where each dictionary contains\\n                           the model\\'s name, score, and metrics.\\n        \"\"\"\\n```\\n\\n## Plotting the Comparison\\n\\nThe plot method generates a visual representation of the comparison between the models. It uses an external visualization library to create graphs that illustrate the performance of each model across various metrics. The mode parameter allows for different modes of visualization.\\n\\n## Usage Example\\n\\nBelow is an example of how to use the LLMComparison class to compare different language models and generate a plot.\\n\\n```python\\nfrom indoxJudge.piplines import LLMComparison\\n\\nmodels = [\\n    {\\n        \\'name\\': \\'Model_1\\',\\n        \\'score\\': 0.50,\\n        \\'metrics\\': {\\n            \\'Faithfulness\\': 0.55,\\n            \\'AnswerRelevancy\\': 1.0,\\n            \\'Bias\\': 0.45,\\n            \\'Hallucination\\': 0.8,\\n            \\'KnowledgeRetention\\': 0.0,\\n            \\'Toxicity\\': 0.0,\\n            \\'precision\\': 0.64,\\n            \\'recall\\': 0.77,\\n            \\'f1_score\\': 0.70,\\n            \\'BLEU\\': 0.11\\n        }\\n    },\\n    {\\n        \\'name\\': \\'Model_2\\',\\n        \\'score\\': 0.61,\\n        \\'metrics\\': {\\n            \\'Faithfulness\\': 1.0,\\n            \\'AnswerRelevancy\\': 1.0,\\n            \\'Bias\\': 0.0,\\n            \\'Hallucination\\': 0.8,\\n            \\'KnowledgeRetention\\': 1.0,\\n            \\'Toxicity\\': 0.0,\\n            \\'precision\\': 0.667,\\n            \\'recall\\': 0.77,\\n            \\'f1_score\\': 0.71,\\n            \\'BLEU\\': 0.14\\n        }\\n    },\\n    {\\n        \\'name\\': \\'Model_3\\',\\n        \\'score\\': 0.050,\\n        \\'metrics\\': {\\n            \\'Faithfulness\\': 1.0,\\n            \\'AnswerRelevancy\\': 1.0,\\n            \\'Bias\\': 0.0,\\n            \\'Hallucination\\': 0.83,\\n            \\'KnowledgeRetention\\': 0.0,\\n            \\'Toxicity\\': 0.0,\\n            \\'precision\\': 0.64,\\n            \\'recall\\': 0.76,\\n            \\'f1_score\\': 0.70,\\n            \\'BLEU\\': 0.10\\n        }\\n    }\\n]\\n\\nllm_comparison = LLMComparison(models=models)\\nllm_comparison.plot(mode=\"inline\")\\n```\\n\\nIn this example, three models are compared based on their metrics, and the results are plotted inline. The LLMComparison class facilitates the easy comparison and visualization of different language models\\' performances.',\n",
       " '# LLMEvaluator\\n\\n## Overview\\n\\nThe `LLMEvaluator` class is designed to evaluate various aspects of language model outputs using specified metrics. It supports metrics such as Faithfulness, Answer Relevancy, Bias, Contextual Relevancy, GEval, Hallucination, Knowledge Retention, Toxicity, BertScore, BLEU, Rouge, and METEOR. This class provides a comprehensive assessment of different dimensions of model performance, making it ideal for thorough evaluations of language models.\\n\\n## Initialization\\n\\nThe `LLMEvaluator` class is initialized with four main components:\\n\\n- **llm_as_judge**: The language model acting as the judge for the evaluation.\\n- **llm_response**: The response generated by the language model.\\n- **retrieval_context**: The context used for retrieval-based metrics.\\n- **query**: The query or prompt provided to the language model.\\n\\n### Example\\n\\n```python\\nclass LLMEvaluator:\\n    def __init__(self, llm_as_judge, llm_response, retrieval_context, query):\\n        \"\"\"\\n        Initializes the Evaluator with a language model and a list of metrics.\\n\\n        Args:\\n            llm_as_judge: The language model acting as the judge for the evaluation.\\n            llm_response: The response generated by the language model.\\n            retrieval_context: The context used for retrieval-based metrics.\\n            query: The query or prompt provided to the language model.\\n        \"\"\"\\n        self.model = llm_as_judge\\n        self.metrics = [\\n            Faithfulness(llm_response=llm_response, retrieval_context=retrieval_context),\\n            AnswerRelevancy(query=query, llm_response=llm_response),\\n            Bias(llm_response=llm_response),\\n            Hallucination(llm_response=llm_response, retrieval_context=retrieval_context),\\n            KnowledgeRetention(messages=[{\"query\": query, \"llm_response\": llm_response}]),\\n            Toxicity(messages=[{\"query\": query, \"llm_response\": llm_response}]),\\n            BertScore(llm_response=llm_response, retrieval_context=retrieval_context),\\n            BLEU(llm_response=llm_response, retrieval_context=retrieval_context),\\n        ]\\n```\\n\\n## Setting the Model for Metrics\\n\\nThe `set_model_for_metrics` method ensures that the language model is properly set for each metric that requires it. This step is crucial for metrics that need direct access to the model for evaluation purposes.\\n\\n### Usage Example\\n\\n```python\\nimport os\\nfrom dotenv import load_dotenv\\n\\n# Load environment variables\\nload_dotenv()\\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\\n\\nfrom indoxJudge.piplines import LLMEvaluator\\nfrom indoxJudge.models import OpenAi\\n\\nquery = \"What are the benefits of a Mediterranean diet?\"\\nretrieval_context = [\\n    \"The Mediterranean diet emphasizes eating primarily plant-based foods, such as fruits and vegetables, whole grains, legumes, and nuts. It also includes moderate amounts of fish and poultry, and low consumption of red meat. Olive oil is the main source of fat, providing monounsaturated fats which are beneficial for heart health.\",\\n    \"Research has shown that the Mediterranean diet can reduce the risk of heart disease, stroke, and type 2 diabetes. It is also associated with improved cognitive function and a lower risk of Alzheimer\\'s disease. The diet\\'s high content of fiber, antioxidants, and healthy fats contributes to its numerous health benefits.\",\\n    \"A Mediterranean diet has been linked to a longer lifespan and a reduced risk of chronic diseases. It promotes healthy aging and weight management due to its emphasis on whole, unprocessed foods and balanced nutrition.\"\\n]\\n\\n# Obtain the model\\'s response\\nresponse = \"The Mediterranean diet is known for its health benefits, including reducing the risk of heart disease, stroke, and diabetes. It encourages the consumption of fruits, vegetables, whole grains, nuts, and olive oil, while limiting red meat. Additionally, this diet has been associated with better cognitive function and a reduced risk of Alzheimer\\'s disease, promoting longevity and overall well-being.\"\\n\\n\\nllm_as_judge = OpenAi(api_key=OPENAI_API_KEY,model=\"gpt-4o\")\\n\\nevaluator = LLMEvaluator(llm_as_judge=llm_as_judge,llm_response=response,retrieval_context=retrieval_context,query=query)\\nllm_results = evaluator.judge()\\n\\nevaluation_score = evaluator.evaluation_score\\nevaluation_metrics_score = evaluator.metrics_score\\n\\n# For plot and visualize (choose inline for using in colab)\\nevaluator.plot(mode=\"external\")\\n```\\n\\nIn this example, the LLMEvaluator class method judge initializes an empty dictionary called results. It iterates through each metric in the metrics list, retrieves the metric\\'s name, and logs the start of the evaluation process. For each metric, the method checks its type and calls the appropriate evaluation method. For instance, the Faithfulness metric involves evaluating claims, truths, and verdicts, which are then stored in the results dictionary under the \\'faithfulness\\' key.\\n\\nIf any exceptions are encountered during the evaluation process, they are caught and logged as errors, and the evaluation continues with the next metric. The final results are returned as a dictionary containing the outcomes of the evaluations for each metric.',\n",
       " '# RagEvaluator\\n\\n## Overview\\n\\nThe `RagEvaluator` class is designed to evaluate various aspects of language model outputs in the context of Retrieval-Augmented Generation (RAG) using specified metrics. It supports metrics such as Faithfulness, Answer Relevancy, Contextual Relevancy, GEval, Hallucination, Knowledge Retention, BertScore, and METEOR. This class provides a comprehensive assessment of different dimensions of RAG model performance, making it ideal for thorough evaluations of retrieval-based language models.\\n\\n## Initialization\\n\\nThe `RagEvaluator` class is initialized with four main components:\\n\\n- **llm_as_judge**: The language model acting as the judge for the evaluation.\\n- **llm_response**: The response generated by the language model.\\n- **retrieval_context**: The context retrieved for the query.\\n- **query**: The query or prompt provided to the language model.\\n\\n### Example\\n\\n```python\\nclass RagEvaluator:\\n    def __init__(self, llm_as_judge, llm_response, retrieval_context, query):\\n        \"\"\"\\n        Initializes the RagEvaluator with a language model and a list of metrics.\\n\\n        Args:\\n            llm_as_judge: The language model acting as the judge for the evaluation.\\n            llm_response: The response generated by the language model.\\n            retrieval_context: The context retrieved for the query.\\n            query: The query or prompt provided to the language model.\\n        \"\"\"\\n        self.model = llm_as_judge\\n        self.metrics = [\\n            Faithfulness(llm_response=llm_response, retrieval_context=retrieval_context),\\n            AnswerRelevancy(query=query, llm_response=llm_response),\\n            ContextualRelevancy(query=query, retrieval_context=retrieval_context),\\n            GEval(parameters=\"Rag Pipeline\", llm_response=llm_response, query=query, retrieval_context=retrieval_context),\\n            Hallucination(llm_response=llm_response, retrieval_context=retrieval_context),\\n            KnowledgeRetention(messages=[{\"query\": query, \"llm_response\": llm_response}]),\\n            BertScore(llm_response=llm_response, retrieval_context=retrieval_context),\\n            METEOR(llm_response=llm_response, retrieval_context=retrieval_context),\\n        ]\\n```\\n## Setting the Model for Metrics\\nThe `set_model_for_metrics` method ensures that the language model is properly set for each metric that requires it. This step is crucial for metrics that need direct access to the model for evaluation purposes.\\n## Judging\\nThe `judge` method evaluates the language model using the provided metrics and returns the results. It processes each metric individually, handling specific evaluation logic for different metric types.\\n## Usage Example\\n```python\\nimport os\\nfrom dotenv import load_dotenv\\n\\n# Load environment variables\\nload_dotenv()\\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\\n\\nfrom indoxJudge.piplines import RagEvaluator\\nfrom indoxJudge.models import OpenAi\\n\\nquery = \"What are the benefits of a Mediterranean diet?\"\\nretrieval_context = [\\n    \"The Mediterranean diet emphasizes eating primarily plant-based foods...\",\\n    \"Research has shown that the Mediterranean diet can reduce the risk of heart disease...\",\\n    \"A Mediterranean diet has been linked to a longer lifespan and a reduced risk of chronic diseases...\"\\n]\\n\\nresponse = \"The Mediterranean diet is known for its health benefits, including reducing the risk of heart disease...\"\\n\\nllm_as_judge = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-4o\")\\n\\nevaluator = RagEvaluator(llm_as_judge=llm_as_judge, llm_response=response, retrieval_context=retrieval_context, query=query)\\nrag_results = evaluator.judge()\\n\\nevaluation_score = evaluator.evaluation_score\\nevaluation_metrics_score = evaluator.metrics_score\\n\\n# For plot and visualize (choose inline for using in colab)\\nevaluator.plot(mode=\"external\")\\n```\\nIn this example, the `RagEvaluator` class method `judge` initializes an empty dictionary called `results`. It iterates through each metric in the `metrics` list, retrieves the metric\\'s name, and logs the start of the evaluation process. For each metric, the method checks its type and calls the appropriate evaluation method. The results are stored in the `results` dictionary under the corresponding metric key.\\nIf any exceptions are encountered during the evaluation process, they are caught and logged as errors, and the evaluation continues with the next metric. The final results are returned as a dictionary containing the outcomes of the evaluations for each metric.\\n\\n## Visualization\\nThe `plot` method allows for visualization of the evaluation results. It creates a graph representation of the RAG Evaluator\\'s performance across different metrics.',\n",
       " '\\n---\\n\\n# SafetyEvaluator\\n\\n## Overview\\n\\nThe `SafetyEvaluator` class is designed to assess various safety-related aspects of a given input using a set of predefined metrics. This class includes metrics such as Fairness, Harmfulness, Privacy, Misinformation, MachineEthics, and StereotypeBias. It provides a structured approach to evaluating these dimensions and visualizing the results, making it suitable for comprehensive safety evaluations of text inputs.\\n\\n## Initialization\\n\\nThe `SafetyEvaluator` class is initialized with two main components:\\n\\n- **model**: The model used for evaluating the safety metrics.\\n- **input**: The input text or sentence that will be evaluated against the safety metrics.\\n\\n### Example\\n\\n```python\\nclass SafetyEvaluator:\\n    def __init__(self, model, input):\\n        \"\"\"\\n        Initializes the SafetyEvaluator with a model and input sentence.\\n\\n        Args:\\n            model: The model used for evaluating the metrics.\\n            input: The input text that will be evaluated.\\n        \"\"\"\\n        self.model = model\\n        self.metrics = [\\n            Fairness(input_sentence=input),\\n            Harmfulness(input_sentence=input),\\n            Privacy(input_sentence=input),\\n            Misinformation(input_sentence=input),\\n            MachineEthics(input_sentence=input),\\n            StereotypeBias(input_sentence=input)\\n        ]\\n        logger.info(\"Evaluator initialized with model and metrics.\")\\n        self.set_model_for_metrics()\\n        self.evaluation_score = 0\\n        self.metrics_score = {}\\n        self.metrics_reasons = {}\\n```\\n\\n## Setting the Model for Metrics\\n\\nThe `set_model_for_metrics` method ensures that the model is properly set for each metric that requires it. This step is crucial for metrics that need direct access to the model to perform their evaluations.\\n\\n---\\n### Usage Example\\n\\n```python\\nimport os\\nfrom dotenv import load_dotenv\\nfrom indoxJudge.piplines.safetyEvaluator.safety_evaluator import *\\nload_dotenv()\\nINDOX_API_KEY = os.getenv(\"INDOX_API_KEY\")\\nfrom indoxJudge.models import IndoxApi\\nmodel = IndoxApi(api_key=INDOX_API_KEY)\\n\\ninput_sentence = \"Most people believe that teamwork is essential for successful project completion, although some individuals may work better independently.\"\\n\\nmetrics_to_evaluate = [\"Fairness\", \"Harmfulness\"]\\n\\nsafety_model = SafetyEvaluator(model=model, input=input_sentence)\\n\\nmetrics_score, metrics_reasons = safety_model.judge()\\n\\n# Print the evaluation results\\nprint(\"Metrics Scores:\")\\nprint(json.dumps(metrics_score, indent=4))\\n\\nprint(\"\\\\nMetrics Reasons:\")\\nprint(json.dumps(metrics_reasons, indent=4))\\nprint(\"Transformed Metrics:\", transformed_metrics)\\n\\n\\n```\\n\\n\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Splitting Content into Manageable Chunks\n",
    "We use the `RecursiveCharacterTextSplitter` to divide the retrieved content into smaller, coherent chunks. This approach ensures that each segment maintains contextual integrity while managing large volumes of text."
   ],
   "id": "236bd3b930a0f860"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:16:51.762899Z",
     "start_time": "2024-08-25T14:16:51.723263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(400,20)\n",
    "content_chunks = splitter.split_text(content)"
   ],
   "id": "835fe1c42c4e31df",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Storing and Indexing Content with Chroma\n",
    "We use the `Chroma` vector store from the `indox.vector_stores` module to store and index the content chunks. By creating a collection named \"sample\" and applying an embedding function (`embed`), we convert each chunk into a vector for efficient retrieval. The `add` method then adds these vectors to the database, enabling scalable and effective search for question-answering tasks."
   ],
   "id": "b6ff6260e010d3d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:16:57.668373Z",
     "start_time": "2024-08-25T14:16:54.339372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.vector_stores import Chroma\n",
    "db = Chroma(collection_name=\"sample\",embedding_function=embed)\n",
    "db.add(docs=content_chunks)"
   ],
   "id": "4636c3ad55b48d94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mStoring documents in the vector store\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEmbedding documents\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mStarting to fetch embeddings for texts using model: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mDocument added successfully to the vector store.\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mDocuments stored successfully\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Querying GitHub Repository Data with Indox\n",
    "\n",
    "With our `GithubReader` system and `Indox` setup complete, we are ready to test it using a sample query. This test will demonstrate how effectively our system can retrieve and process information from files in the GitHub repository.\n",
    "\n",
    "We’ll use a sample query to evaluate our system:\n",
    "\n",
    "- **Query**: \"What are the guidelines for creating a pull request?\"\n",
    "\n",
    "This question will be processed by the `GithubReader` and `Indox` system to retrieve relevant files and generate a precise response based on the repository content.\n",
    "\n",
    "Let’s test our setup with this query."
   ],
   "id": "53ae63637952ed19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:17:01.440266Z",
     "start_time": "2024-08-25T14:17:01.433268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"What are the guidelines for creating a pull request?\"\n",
    "retriever = indox.QuestionAnswer(vector_database=db, llm=mistral_qa, top_k=1)"
   ],
   "id": "1aac96d2e2ce09f1",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that our `GithubReader` system with `Indox` is fully set up, we can test it with a sample query. We’ll use the invoke method to get a response from the system.\n",
    "\n",
    "The `invoke` method processes the query using the connected QA model and retrieves relevant information from the repository content. \n",
    "\n",
    "We’ll pass the query to the invoke method and print the response to evaluate how effectively the system retrieves and generates answers based on the GitHub repository content."
   ],
   "id": "56f7683ee742a9fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:17:06.627424Z",
     "start_time": "2024-08-25T14:17:04.502863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "answer = retriever.invoke(query)\n",
    "context = retriever.context"
   ],
   "id": "5014b25220fc3dbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mRetrieving context and scores from the vector database\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEmbedding documents\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mStarting to fetch embeddings for texts using model: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mGenerating answer without document relevancy filter\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mAnswering question\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mSending request to Hugging Face API\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mReceived successful response from Hugging Face API\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mQuery answered successfully\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:17:07.450298Z",
     "start_time": "2024-08-25T14:17:07.443195Z"
    }
   },
   "cell_type": "code",
   "source": "answer",
   "id": "2ca2a9e3a80d8857",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To create a pull request, ensure that your code follows a set of technical guidelines beforehand. This includes adhering to coding standards and passing all necessary tests. Additionally, write detailed descriptions for the pull request, explaining the issue solved and what actions you took.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
