{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How to Use ArxivReader for Retrieving Papers from arXiv\n",
    "In this notebook, we will demonstrate how to use the `ArxivReader` class for accessing papers from the arXiv repository. The `ArxivReader` class is designed to interact with the arXiv API and retrieve paper content and metadata, which can be utilized in various research and question-answering systems. We'll be leveraging open-source models available on the internet, such as Mistral, to process the retrieved data.\n",
    "\n",
    "To begin, ensure you have set up your environment variables and API keys in Python using the dotenv library. This is crucial for securely managing sensitive information, such as API keys, especially when using services like HuggingFace. Ensure your `HUGGINGFACE_API_KEY` is defined in the `.env` file to avoid hardcoding sensitive data into your codebase, thus enhancing security and maintainability.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDox/blob/master/Demo/arxiv_chroma_semantic.ipynb)"
   ],
   "id": "33c833fa16351fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install indox\n",
    "!pip install chromadb\n",
    "!pip install arxiv\n",
    "!pip install semantic_text_splitter\n",
    "!pip install sentence_transformers"
   ],
   "id": "97c1731b1cfd21d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setting Up the Python Environment\n",
    "\n",
    "If you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indox`:\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "```bash\n",
    "python -m venv indox\n",
    "```\n",
    "2. **Activate the virtual environment:**\n",
    "```bash\n",
    "indox_judge\\Scripts\\activate\n",
    "```\n",
    "\n",
    "### macOS/Linux\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "   ```bash\n",
    "   python3 -m venv indox\n",
    "    ```\n",
    "\n",
    "2. **Activate the virtual environment:**\n",
    "    ```bash\n",
    "   source indox/bin/activate\n",
    "    ```\n",
    "   \n",
    "### Install Dependencies\n",
    "\n",
    "Once the virtual environment is activated, install the required dependencies by running:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ],
   "id": "b822f39ef478838e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Import Essential Libraries\n",
    "\n",
    "Next, we import the essential libraries for our Indox question-answering system:\n",
    "\n",
    "- `IndoxRetrievalAugmentation`: Enhances the retrieval process by improving the relevance and quality of the documents retrieved, leading to better QA performance.\n",
    "- `MistralQA`: A powerful QA model provided by Indox, built on top of the Hugging Face model architecture. It leverages state-of-the-art language understanding to deliver precise answers.\n",
    "- `HuggingFaceEmbedding`: This library uses Hugging Face embeddings to enrich semantic understanding, making it easier to capture the contextual meaning of the text.\n",
    "- `SemanticTextSplitter`: utilizes a Hugging Face tokenizer to intelligently split text into chunks based on a specified maximum number of tokens, ensuring that each chunk maintains semantic coherence."
   ],
   "id": "8620c6d0c74b5fbe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T13:36:05.951160Z",
     "start_time": "2024-09-05T13:36:05.943943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('api.env')\n",
    "\n",
    "HUGGINGFACE_API_KEY = os.environ['HUGGINGFACE_API_KEY']"
   ],
   "id": "dfc9daa14417b6e9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-05T13:36:11.718603Z",
     "start_time": "2024-09-05T13:36:11.570537Z"
    }
   },
   "source": [
    "from indox import IndoxRetrievalAugmentation\n",
    "indox = IndoxRetrievalAugmentation()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mIndoxRetrievalAugmentation initialized\u001B[0m\n",
      "\n",
      "            ██  ███    ██  ██████   ██████  ██       ██\n",
      "            ██  ████   ██  ██   ██ ██    ██   ██  ██\n",
      "            ██  ██ ██  ██  ██   ██ ██    ██     ██\n",
      "            ██  ██  ██ ██  ██   ██ ██    ██   ██   ██\n",
      "            ██  ██  █████  ██████   ██████  ██       ██\n",
      "            \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Building the ArxivReader System and Initializing Models\n",
    "\n",
    "Next, we will build our ArxivReader system and initialize the MistralQA model along with the HuggingFaceEmbedding model. This setup will enable us to effectively retrieve and process arXiv papers, leveraging the advanced capabilities of these models for our question-answering tasks.\n"
   ],
   "id": "e014e14383ad4fbd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T13:36:25.897567Z",
     "start_time": "2024-09-05T13:36:15.357657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.llms import HuggingFaceModel\n",
    "from indox.embeddings import HuggingFaceEmbedding\n",
    "mistral_qa = HuggingFaceModel(api_key=HUGGINGFACE_API_KEY,model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "embed = HuggingFaceEmbedding(api_key=HUGGINGFACE_API_KEY,model=\"multi-qa-mpnet-base-cos-v1\")"
   ],
   "id": "aad9a7a29fc81465",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mInitializing HuggingFaceModel with model: mistralai/Mistral-7B-Instruct-v0.2\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mHuggingFaceModel initialized successfully\u001B[0m\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "tokenizers>=0.19,<0.20 is required for a normal functioning of this module, but found tokenizers==0.15.2.\nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mindox\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01membeddings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HuggingFaceEmbedding\n\u001B[0;32m      3\u001B[0m mistral_qa \u001B[38;5;241m=\u001B[39m HuggingFaceModel(api_key\u001B[38;5;241m=\u001B[39mHUGGINGFACE_API_KEY,model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmistralai/Mistral-7B-Instruct-v0.2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m embed \u001B[38;5;241m=\u001B[39m \u001B[43mHuggingFaceEmbedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mapi_key\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mHUGGINGFACE_API_KEY\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmulti-qa-mpnet-base-cos-v1\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\My Files\\Indox\\New folder\\inDox\\indox\\embeddings\\huggingface.py:15\u001B[0m, in \u001B[0;36mHuggingFaceEmbedding.__init__\u001B[1;34m(self, model, api_key)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, model: \u001B[38;5;28mstr\u001B[39m, api_key: \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m---> 15\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SentenceTransformer\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m model\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key \u001B[38;5;241m=\u001B[39m api_key\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject4\\.venv\\Lib\\site-packages\\sentence_transformers\\__init__.py:7\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mimportlib\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcross_encoder\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mCrossEncoder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CrossEncoder\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ParallelSentencesDataset, SentencesDataset\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mLoggingHandler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoggingHandler\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject4\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mCrossEncoder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CrossEncoder\n\u001B[0;32m      3\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCrossEncoder\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject4\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:12\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoader\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautonotebook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm, trange\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, is_torch_npu_available\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenization_utils_base\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BatchEncoding\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PushToHubMixin\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject4\\.venv\\Lib\\site-packages\\transformers\\__init__.py:26\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dependency_versions_check\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     28\u001B[0m     OptionalDependencyNotAvailable,\n\u001B[0;32m     29\u001B[0m     _LazyModule,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     48\u001B[0m     logging,\n\u001B[0;32m     49\u001B[0m )\n\u001B[0;32m     52\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mget_logger(\u001B[38;5;18m__name__\u001B[39m)  \u001B[38;5;66;03m# pylint: disable=invalid-name\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject4\\.venv\\Lib\\site-packages\\transformers\\dependency_versions_check.py:57\u001B[0m\n\u001B[0;32m     54\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_accelerate_available():\n\u001B[0;32m     55\u001B[0m             \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# not required, check version only if installed\u001B[39;00m\n\u001B[1;32m---> 57\u001B[0m     \u001B[43mrequire_version_core\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdeps\u001B[49m\u001B[43m[\u001B[49m\u001B[43mpkg\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpkg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdeps\u001B[38;5;241m.\u001B[39mkeys()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, check dependency_versions_table.py\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject4\\.venv\\Lib\\site-packages\\transformers\\utils\\versions.py:117\u001B[0m, in \u001B[0;36mrequire_version_core\u001B[1;34m(requirement)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"require_version wrapper which emits a core-specific hint on failure\"\"\"\u001B[39;00m\n\u001B[0;32m    116\u001B[0m hint \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTry: `pip install transformers -U` or `pip install -e \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.[dev]\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m` if you\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mre working with git main\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 117\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrequire_version\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequirement\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhint\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject4\\.venv\\Lib\\site-packages\\transformers\\utils\\versions.py:111\u001B[0m, in \u001B[0;36mrequire_version\u001B[1;34m(requirement, hint)\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m want_ver \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    110\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m op, want_ver \u001B[38;5;129;01min\u001B[39;00m wanted\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m--> 111\u001B[0m         \u001B[43m_compare_versions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgot_ver\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwant_ver\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequirement\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpkg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhint\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject4\\.venv\\Lib\\site-packages\\transformers\\utils\\versions.py:44\u001B[0m, in \u001B[0;36m_compare_versions\u001B[1;34m(op, got_ver, want_ver, requirement, pkg, hint)\u001B[0m\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     40\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to compare versions for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrequirement\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: need=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwant_ver\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m found=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgot_ver\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. This is unusual. Consider\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     41\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m reinstalling \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpkg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     42\u001B[0m     )\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ops[op](version\u001B[38;5;241m.\u001B[39mparse(got_ver), version\u001B[38;5;241m.\u001B[39mparse(want_ver)):\n\u001B[1;32m---> 44\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m     45\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrequirement\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is required for a normal functioning of this module, but found \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpkg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m==\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgot_ver\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhint\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     46\u001B[0m     )\n",
      "\u001B[1;31mImportError\u001B[0m: tokenizers>=0.19,<0.20 is required for a normal functioning of this module, but found tokenizers==0.15.2.\nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setting Up the ArxivReader for Retrieving Papers\n",
    "To demonstrate the capabilities of our `ArxivReader` system and `Indox` question-answering model, we will use a sample paper ID. This paper will contain arXiv paper, which we will use for testing and evaluation."
   ],
   "id": "f0032f294a4379f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:30:04.837027Z",
     "start_time": "2024-08-27T07:30:02.598380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.data_connectors import ArxivReader\n",
    "\n",
    "reader = ArxivReader()\n",
    "\n",
    "paper_ids = [\"2201.08239\"]\n",
    "documents = reader.load_content(paper_ids)"
   ],
   "id": "ff2ba25cec5cb1a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:30:07.057854Z",
     "start_time": "2024-08-27T07:30:07.049857Z"
    }
   },
   "cell_type": "code",
   "source": "content = documents",
   "id": "2d3e6113f3f5e1ca",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Splitting Content into Manageable Chunks\n",
    "We use the `SemanticTextSplitter` function from the `indox.splitter` module to divide the retrieved content into smaller, meaningful chunks."
   ],
   "id": "5139a5ed0a1e734b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:30:09.592094Z",
     "start_time": "2024-08-27T07:30:09.214442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.splitter import SemanticTextSplitter\n",
    "splitter = SemanticTextSplitter(400)\n",
    "content_chunks = splitter.split_text(content)"
   ],
   "id": "835fe1c42c4e31df",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Storing and Indexing Content with Chroma\n",
    "We use the `Chroma` vector store from the `indox.vector_stores` module to store and index the content chunks. By creating a collection named \"sample\" and applying an embedding function (`embed`), we convert each chunk into a vector for efficient retrieval. The `add` method then adds these vectors to the database, enabling scalable and effective search for question-answering tasks."
   ],
   "id": "6b882f2c8f21fec0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:30:14.477310Z",
     "start_time": "2024-08-27T07:30:12.967248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.vector_stores import Chroma\n",
    "db = Chroma(collection_name=\"sample\",embedding_function=embed)\n",
    "db.add(docs=content_chunks)"
   ],
   "id": "4636c3ad55b48d94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mStoring documents in the vector store\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEmbedding documents\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mStarting to fetch embeddings for texts using model: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mDocument added successfully to the vector store.\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mDocuments stored successfully\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Querying the Arxiv Data with Indox\n",
    "With our `ArxivReader` system and `Indox` setup complete, we are ready to test it using a sample query. This test will show how well our system can retrieve and generate accurate answers based on the arXiv papers stored in the vector store.\n",
    "\n",
    "We’ll use a sample query to evaluate our system:\n",
    "- **Query**: \"what are challenges?\"\n",
    "\n",
    "This question will be processed by the `ArxivReader` and `Indox` system to retrieve relevant papers and generate a precise response based on the information.\n",
    "\n",
    "Let’s test our setup with this query."
   ],
   "id": "d6c8870e944d72a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:30:18.173362Z",
     "start_time": "2024-08-27T07:30:18.167379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"what are challenges?\"\n",
    "retriever = indox.QuestionAnswer(vector_database=db, llm=mistral_qa, top_k=2)"
   ],
   "id": "a9e7053437d60ca",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that our `ArxivReader` system with `Indox` is fully set up, we can test it with a sample query. We’ll use the `invoke` method to get a response from the system.\n",
    "The `invoke` method processes the query using the connected QA model and retrieves relevant information from the vector store.\n",
    "\n",
    "We’ll pass the query to the invoke method and print the response to evaluate how effectively the system retrieves and generates answers."
   ],
   "id": "7e3e8bd67f9e7239"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:30:23.412961Z",
     "start_time": "2024-08-27T07:30:20.862778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "answer = retriever.invoke(query)\n",
    "context = retriever.context"
   ],
   "id": "cad8c3510343e8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mRetrieving context and scores from the vector database\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEmbedding documents\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mStarting to fetch embeddings for texts using model: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mGenerating answer without document relevancy filter\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mAnswering question\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mSending request to Hugging Face API\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mReceived successful response from Hugging Face API\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mQuery answered successfully\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:30:24.949269Z",
     "start_time": "2024-08-27T07:30:24.936267Z"
    }
   },
   "cell_type": "code",
   "source": "answer",
   "id": "a1edb80591b75581",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The challenges addressed in the article are safety and factual grounding for LaMDA, a family of Transformer-based neural language models specialized for dialog applications. Safety challenges include ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. Factual grounding challenges involve enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
